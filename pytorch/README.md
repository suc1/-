# torch
1. deepshare.net
2. [60AIç»å…¸è®ºæ–‡](https://deepshare.feishu.cn/docs/doccnewbeOX1q1t4Pk5npjv0p5d)
3. [æ·±åº¦å­¦ä¹ è·¯çº¿å›¾](https://www.zhihu.com/question/437199981/answer/3310028730)

## env
1. `conda create --name pytorchTest opencv matplotlib -y`
2. `conda activate pytorchTest`
3. `conda install -c conda-forge pytorch-cpu torchvision tensorboard future`
4. `conda install psutil`
5. `tensorboard --logdir=./runs`
6. `pip install torchsummary`
7. Install `libjpeg` or `libpng`:
	1. `conda install Pillow`
	2. Linux: `sudo apt-get install libjpeg-dev libpng-dev`
	3. `conda uninstall torchvision`
	4. `conda install torchvision`



## version
```
torch.__version__
torch.cuda.is_available()
torch.version.cuda
torch.cuda.get_device_name(0)
```

## Tensor
```
torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)
torch.from_numpy(ndarray)		# tensor.data å…±äº«å†…å­˜

torch.zeros (*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)	# åªæ˜¯å½¢çŠ¶ä¸€è‡´

w.grad.zero_()		#Reset tensor.grad
x.data.numpy()

torch.ones()
torch.ones_like()

torch.full (size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.full_like()

torch.arange( start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) # ç­‰å·®æ•°åˆ—
torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) # å‡åˆ†
torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False )

torch.normal (mean, std, out=None)																	# æ­£æ€åˆ†å¸ƒ
torch.normal (mean, std, size, out=None)
torch.randn (*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 	#æ ‡å‡†æ­£æ€åˆ†å¸ƒ
torch.randn_like ()

torch.rand()
torch.rand_like()		# å‡åŒ€åˆ†å¸ƒ
torch.randint()
torch.randint_like()

torch.randperm()		# å‡åŒ€ä¸€ç»´å¼ é‡
torch.bernoulli(input, *, generator=None, out=None)
```

## å¼ é‡æ“ä½œ
```
torch.cat (tensors, dim=0, out=None)		# åœ¨æŒ‡å®šç»´åº¦ dim ä¸Šè¿›è¡Œæ‰©å……
torch.stack(tensors, dim=0, out=None)		# åœ¨æ–°åˆ›å»ºçš„ç»´åº¦ dim ä¸Šè¿›è¡Œæ‹¼æ¥

torch.chunk( input, chunks, dim=0)
torch.split(tensor, split_size_or_sections, dim=0)

torch.index_select( input, dim, index, out=None)
t.le(5)		#ge(), gt(), lt()
torch.masked_select( input, mask, out=None)		# ç»“æœæ˜¯ä¸€ç»´

torch.reshape( input, shape)		# å…±äº«æ•°æ®å†…å­˜
torch.transpose( input, dim0, dim1)
torch.t( input )

torch.squeeze( input, dim=None, out=None)
torch.usqueeze( input, dim, out=None)

torch.add()			# out = input + alpha Ã— other
torch.addcdiv()		# out = input + value * tensor1 / tensor2
torch.addcmul()		# out = input + value * tensor1 * tensor2
torch.sub()
torch.div()
torch.mul()

torch.mm()          # çŸ©é˜µç›¸ä¹˜ï¼Œä¸ä¼ æ’­
torch.matmul()      # çŸ©é˜µç›¸ä¹˜ï¼Œä¼ æ’­

torch.log( input , out=None)
torch.log10( input , out=None)
torch.log2( input , out=None)
torch.exp( input , out=None)
torch.pow()

torch.abs( input , out=None)
torch.acos( input , out=None)
torch.cosh( input , out=None)			#
torch.cos( input , out=None)
torch.asin( input , out=None)
torch.atan( input , out=None)
torch.atan2( input , other, out=None)	#
```

## çº¿æ€§å›å½’ Linear Regression
1. çº¿æ€§å…³ç³»: y = wx + b
2. æ±‚è§£æ­¥éª¤: ç¡®å®šæ¨¡å‹ï¼Œé€‰æ‹©æŸå¤±å‡½æ•°ï¼Œæ±‚è§£æ¢¯åº¦å¹¶æ›´æ–°: w = w - LR * w.grad, b = b - LR * w.grad, 

## è®¡ç®—å›¾ä¸åŠ¨æ€å›¾æœºåˆ¶
1. ç»“ç‚¹è¡¨ç¤ºæ•°æ®, è¾¹è¡¨ç¤ºè¿ç®—
2. æ¢¯åº¦æ±‚å¯¼: æ¯æ¡è·¯å¾„ä¹‹é—´åˆ†æ®µæ˜¯ç›¸ä¹˜å…³ç³»ï¼Œæ‰€æœ‰è·¯å¾„ä¹‹é—´æ˜¯ç›¸åŠ å…³ç³»ï¼ˆæ±‚å¯¼æ³•åˆ™ï¼‰
3. `torch.Tensor`: data, dtype, shape, device, requires_grad, grad, grad_fn, is_leaf
4. åŠ¨æ€å›¾, é™æ€å›¾

## autograd è‡ªåŠ¨æ±‚å¯¼ç³»ç»Ÿ
1. `torch.autograd.backward( tensors, grad_tensors=None, retain_graph=None, create_graph=False)`
2. `torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False)` æ±‚å–æ¢¯åº¦
    1. `outputs`è¿›è¡Œæ±‚å¯¼è¿ç®—
	2. `inputs`æ±‚å¯¼ä»¥åå¸¦å…¥å€¼è®¡ç®—æœ€åå€¼
	3. æ¢¯åº¦ä¸è‡ªåŠ¨æ¸…é›¶
    4. ä¾èµ–äºå¶å­ç»“ç‚¹çš„ç»“ç‚¹ï¼Œ requires_grad é»˜è®¤ä¸º True
    5. å¶å­ç»“ç‚¹ä¸å¯æ‰§è¡Œ in place
3. é€»è¾‘å›å½’ Logistic Regression
    1. çº¿æ€§ çš„ äºŒåˆ†ç±»
    2. Sigmoid å‡½æ•°ï¼Œä¹Ÿç§°ä¸ºLogisticå‡½æ•°, (0, 1)
    3. çº¿æ€§å›å½’ `y = Wx + b`
    4. å¯¹æ•°å›å½’ `lny = Wx + b`
    5. å¯¹æ•°å‡ ç‡å›å½’ `ln(y/(1-y)) = Wx + b`
4. è¿­ä»£è®­ç»ƒæ­¥éª¤: æ•°æ®ï¼Œ æ¨¡å‹ï¼Œ æŸå¤±å‡½æ•°ï¼Œ ä¼˜åŒ–å™¨

## DataLoaderä¸Dataset
1. æ•°æ®: æ”¶é›†, åˆ’åˆ†(Train, Test), è¯»å–(DataLoader, DataSet), é¢„å¤„ç†(transforms)
2. `DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=False)`
3. `epoch` = `iteration` * `batchSize`
4. `Dataset.__getitem__()`
5. `DataLoader`:
	1. è¯»å“ªäº›æ•°æ®: Samplerè¾“å‡ºçš„ Index
	2. ä»å“ªè¯»æ•°æ®: Datasetä¸­çš„ data_dir
	3. æ€ä¹ˆè¯»æ•°æ®: Datasetä¸­çš„ __getitem__
6. DataLoader -> DataLoaderIter -> Sampler (-> Index) -> DatasetFetcher -> Dataset -> getitem -> (Img, Label) -> collate_fn -> (Batch Data)
7. `BASE_DIR = os.path.dirname(os.path.abspath(__file__))`
8. `os.path`: `abspath()`, `exists()`
9. åˆ†æˆä¸‰ç»„: train, valid, test. æŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼Œå„ä¸ªé›†ä¸ç›¸äº¤
```
	train_pct = 0.8
    valid_pct = 0.1
    test_pct = 0.1

    for root, dirs, files in os.walk(dataset_dir):	# åªå¤„ç†å½“å‰ç›®å½•ï¼Œä¸è¿›å…¥ä¸‹é¢çš„å­ç›®å½•
        for sub_dir in dirs:

            imgs = os.listdir(os.path.join(root, sub_dir))
            imgs = list(filter(lambda x: x.endswith('.jpg'), imgs))
            random.shuffle(imgs)
            img_count = len(imgs)

            train_point = int(img_count * train_pct)
            valid_point = int(img_count * (train_pct + valid_pct))

            if img_count == 0:
                print("{}ç›®å½•ä¸‹ï¼Œæ— å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥".format(os.path.join(root, sub_dir)))
                import sys
                sys.exit(0)
            for i in range(img_count):
                if i < train_point:
                    out_dir = os.path.join(train_dir, sub_dir)
                elif i < valid_point:
                    out_dir = os.path.join(valid_dir, sub_dir)
                else:
                    out_dir = os.path.join(test_dir, sub_dir)

                makedir(out_dir)

                target_path = os.path.join(out_dir, imgs[i])
                src_path = os.path.join(dataset_dir, sub_dir, imgs[i])

                shutil.copy(src_path, target_path)
```
10. å¢åŠ pythonæœç´¢è·¯å¾„`sys.path.append(hello_pytorch_DIR)`

## å›¾åƒé¢„å¤„ç† transforms
1. `torchvision.transforms`, `torchvision.datasets`, `torchvision.model`
2. é¢„å¤„ç†æ–¹æ³•: æ•°æ®ä¸­å¿ƒåŒ–, æ•°æ®æ ‡å‡†åŒ–, ç¼©æ”¾, è£å‰ª, æ—‹è½¬, ç¿»è½¬, å¡«å……, å™ªå£°æ·»åŠ , ç°åº¦å˜æ¢, çº¿æ€§å˜æ¢, ä»¿å°„å˜æ¢, äº®åº¦ã€é¥±å’Œåº¦åŠå¯¹æ¯”åº¦å˜æ¢
3. å¯ä»¥æ”¾åˆ°æ•°æ®æ¸…æ´—é˜¶æ®µ: ??
4. åœ¨`getitem()`ä¹‹åçš„`transforms`æ˜¯ä¸€å˜å¤š??
5. æ ‡å‡†åŒ–: `transforms.Normalize()`æ˜¯æ‰€æœ‰é€šé“çš„`mean`,`std`. æ€ä¹ˆé€‰æ‹©: å–å†³äºæ•°æ®é›†çš„ç‰¹æ€§

## å›¾åƒå¢å¼º
1. è£å‰ª: `transforms.CenterCrop()`, `transforms.RandomCrop()`, `RandomResizedCrop()`, `FiveCrop()`, `TenCrop()`
2. ç¿»è½¬: `RandomHorizontalFlip()`, `RandomVerticalFlip()`
3. æ—‹è½¬: `RandomRotation()`
4. å›¾åƒå˜æ¢: `Pad()`
4. è°ƒæ•´äº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å’Œè‰²ç›¸: `ColorJitter()`. æ ‡å‡†åŒ–ä»¥åï¼Œä¸ç”¨è¿™ä¸ª???
5. ä¾æ¦‚ç‡å°†å›¾ç‰‡è½¬æ¢ä¸ºç°åº¦å›¾: `Grayscale()`, `RandomGrayscale()`
6. `RandomAffine()`, `transforms.LinearTransformation`, `RandomErasing()`, `transforms.Lambda()`
7. `transforms.Resize`, `transforms.Totensor`, `transforms.Normalize`
8. `transforms.RandomChoice()`, `transforms.RandomApply`, `transforms.RandomOrder()`
9. è‡ªå®šä¹‰transforms: `__init__`ä¼ å…¥å‚æ•°ï¼Œ`__call__`åªæœ‰ä¸€ä¸ªå›¾ç‰‡å‚æ•°. æ¤’ç›å™ªå£°
10. åº”ç”¨(åŸåˆ™ï¼šè®©è®­ç»ƒé›†ä¸æµ‹è¯•é›†/ç°å®æ›´æ¥è¿‘): ç©ºé—´ä½ç½® ï¼šå¹³ç§»; è‰²å½© ï¼šç°åº¦å›¾ï¼Œè‰²å½©æŠ–åŠ¨; å½¢çŠ¶ ï¼šä»¿å°„å˜æ¢; ä¸Šä¸‹æ–‡åœºæ™¯ ï¼šé®æŒ¡ï¼Œå¡«å……
11. `if isinstance(m, nn.Conv2d)`
12. è®­ç»ƒæ¨¡å¼`net.train()`, è¯„ä»·æ¨¡å¼`net.eval()`, ä½¿ç”¨æ¨¡å¼
13. åˆšä½“å˜æ¢ï¼ˆRigid Transformationï¼‰: å¹³ç§»ï¼ˆTranslationï¼‰å’Œæ—‹è½¬ï¼ˆRotationï¼‰. ç‰©ä½“çš„å¤§å°å’Œå½¢çŠ¶ä¸å˜
14. ä»¿å°„å˜æ¢ï¼ˆAffine Transformationï¼‰ï¼šåˆšä½“å˜æ¢ + ç¼©æ”¾/ç¿»è½¬/å‰ªåˆ‡ï¼ˆShearï¼‰. ç›´çº¿ä»ä¸ºç›´çº¿, å¹³è¡Œçº¿ä»ä¸ºå¹³è¡Œçº¿
15. æŠ•å½±å˜æ¢ï¼ˆProjective Transformationï¼‰: ä»¿å°„å˜æ¢ + é€è§†å˜æ¢(3D -> 2D). 
16. åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé€šå¸¸åŒä¸€å¼ å›¾ç‰‡ä¼šè¢«è¾“å…¥æ¨¡å‹å¤šæ¬¡ï¼Œè€Œä¸æ˜¯ä»…è¾“å…¥ä¸€æ¬¡ã€‚è¿™ä¸€æŠ€æœ¯é€šå¸¸è¢«ç§°ä¸ºæ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰: åŒ…æ‹¬éšæœºæ—‹è½¬ã€æ°´å¹³ç¿»è½¬ã€å‚ç›´ç¿»è½¬ã€ç¼©æ”¾ã€å¹³ç§»ç­‰. å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·, æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
17. åœ¨ä¸åŒçš„`epoch`ä¸­ï¼Œå³ä½¿æ²¡æœ‰å›¾ç‰‡çš„éšæœºå˜æ¢ï¼ŒåŒä¸€ä¸ªå›¾ç‰‡è¿˜æ˜¯è¢«è¾“å…¥å¤šæ¬¡
18. `with torch.no_grad():` ç”¨äºæŒ‡å®šåœ¨å…¶å†…éƒ¨çš„ä»£ç å—ä¸­ä¸è®¡ç®—æ¢¯åº¦ã€‚ç”¨äºéªŒè¯/æ¨æ–­ï¼ˆinferenceï¼‰,å¯ä»¥æé«˜ä»£ç çš„æ‰§è¡Œæ•ˆç‡ï¼Œå¹¶å‡å°‘å†…å­˜ä½¿ç”¨

## å·ç§¯æ ¸çš„æ•°é‡æˆ–å·ç§¯æ ¸çš„æ·±åº¦
1. `self.conv1 = nn.Conv2d(3, 6, 5)`è¡¨ç¤ºè¾“å…¥é€šé“æ•°ä¸º 3ï¼Œè¾“å‡ºé€šé“æ•°ä¸º 6ï¼Œå·ç§¯æ ¸å¤§å°ä¸º 5x5
2. è¾“å‡ºé€šé“æ•°6ï¼Œä¹Ÿå«åšå·ç§¯æ ¸çš„æ•°é‡æˆ–å·ç§¯æ ¸çš„æ·±åº¦
3. æ¯ä¸ªå·ç§¯æ ¸éƒ½åœ¨æ‰€æœ‰è¾“å…¥é€šé“ä¸Šé€é€šé“è¿›è¡Œå·ç§¯æ“ä½œ, å¹¶å°†3é€šé“ç»“æœç›¸åŠ ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªè¾“å‡ºé€šé“çš„ç»“æœ
4. å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªå·ç§¯æ ¸ï¼Œä¸è®ºè¾“å…¥æ˜¯å‡ ä¸ªé€šé“, æœ€ç»ˆéƒ½ä¼šäº§ç”Ÿä¸€ä¸ªè¾“å‡ºé€šé“
5. åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œå·ç§¯æ ¸çš„å¤§å°é€šå¸¸æŒ‡çš„æ˜¯åœ¨ä¸€ä¸ªé€šé“ä¸Šçš„å¤§å°ï¼Œè€Œä¸æ˜¯è€ƒè™‘æ‰€æœ‰é€šé“ã€‚å› æ­¤ï¼Œåœ¨ `nn.Conv2d(3, 6, 5)` ä¸­ï¼Œå·ç§¯æ ¸çš„å¤§å°æ˜¯æŒ‡åœ¨æ¯ä¸ªè¾“å…¥é€šé“ä¸Šçš„å¤§å°ä¸º `5x5`
6. æœ‰ 6 ä¸ªè¿™æ ·çš„å·ç§¯æ ¸ï¼Œæ¯ä¸ªå·ç§¯æ ¸éƒ½æ˜¯å¤§å°ä¸º 5x5 çš„çŸ©é˜µã€‚è¿™ 6 ä¸ªå·ç§¯æ ¸åˆ†åˆ«ä¸è¾“å…¥æ•°æ®è¿›è¡Œå·ç§¯ï¼Œäº§ç”Ÿ 6 ä¸ªè¾“å‡ºé€šé“
7. æŸ¥çœ‹è¿™äº›å·ç§¯æ ¸çš„å…·ä½“æ•°å€¼: `print(self.conv1.weight)`
8. `self.fc1 = nn.Linear(16 * 5 * 5, 120)`ä¸­çš„`16 * 5 * 5`æ¥è‡ªäºå‰ä¸¤ä¸ªå·ç§¯å±‚çš„`è¾“å‡ºé€šé“ * è¾“å‡ºå½¢çŠ¶`
9. `torch.nn.Linear(in_features, out_features, bias=True)`

## lesson-06/2_train_lenet.py
1. è¾“å…¥æ•°æ®çš„å½¢çŠ¶ä¸º `(batch_size, 3, height, width)`, å³`(bs, 3, 32, 32)`
2. ç»è¿‡`class LeNet(nn.Module).forward()`çš„å„ä¸ªæ­¥éª¤:
3. `nn.Conv2d(3, 6, 5)`: `(bs, 6, 28, 28)`, å…¶ä¸­`28=32-5+1`
4. `F.max_pool2d(out, 2)`: `(bs, 6, 14, 14)`
5. `nn.Conv2d(6, 16, 5)`: `(bs, 16, 10, 10)`
6. `F.max_pool2d(out, 2)`: `(bs, 16, 5, 5)`
7. `nn.Linear(16*5*5, 120)`: `(bs, 120)`

## æœ‰ç”¨å‡½æ•°
```
# tools/common_tools.py
def set_seed(seed=1)
def get_memory_info()
def transform_invert(img_, transform_train)	

# model/lenet.py
def forward(self, x)			# ç»è¿‡å¤šå±‚å¤„ç†
def initialize_weights(self)	# nn.Conv2d, nn.BatchNorm2d, nn.Linear
class LeNet2(nn.Module)			# features, classifier

l = torch.tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0])
len(l)							# 16
l.size()						# torch.Size([16])
l.size(0)						# 16
a = l.size()
a[0]
l.shape							# torch.Size([16])
l.shape[0]						# 16
l.shape(0)						# TypeError: 'torch.Size' object is not callable

a = torch.tensor([False,  True, False, False, False,  True,  True, False, False, False,
         True,  True, False, False,  True, False])		# False=0; True=1
a.sum()							# tensor(6)
a.sum().numpy()					# array(6, dtype=int64)

p = [0, 1]
rmb = 1 if p[0] == 0 else 100	# C++ # rmb = (p[0] == 0) ? 1 : 100
```

## æ¨¡å‹åˆ›å»ºä¸ nn.Module
1. æ¨¡å‹åˆ›å»º: æ„å»ºç½‘ç»œå±‚(å·ç§¯å±‚ï¼Œæ± åŒ–å±‚ï¼Œæ¿€æ´»å‡½æ•°å±‚ç­‰), æ‹¼æ¥ç½‘ç»œå±‚(LeNet, AlexNet, ResNet)
2. æƒå€¼åˆå§‹åŒ–: Xavier, Kaiming, å‡åŒ€åˆ†å¸ƒï¼Œæ­£æ€åˆ†å¸ƒç­‰
3. æ¨¡å‹æ„å»ºä¸¤è¦ç´ : `__init__()`, `forward()`=æ‹¼æ¥å­æ¨¡å—
4. `torch.nn`: nn.Parameter, nn.Module, nn.functional()=(å·ç§¯ï¼Œæ± åŒ–ï¼Œæ¿€æ´»å‡½æ•°ç­‰), nn.init()=(å‚æ•°åˆå§‹åŒ–æ–¹æ³•)
5. nn.Module(8 ä¸ªå­—å…¸ç®¡ç†å®ƒçš„å±æ€§): parameters, modules, buffers, ***_hooks(å…±5ä¸ªé’©å­)

## æ¨¡å‹å®¹å™¨ä¸ AlexNet æ„å»º
1. å®¹å™¨: nn.Sequetial, nn.ModuleList=(å¯ä»¥å¢åŠ æ¨¡å‹ï¼Œè‡ªå®šä¹‰æ¯ä¸ªå±‚ä¹‹é—´çš„è¿æ¥é€»è¾‘), nn.ModuleDict
2. ä¸¤ä¸ªé˜¶æ®µ: features, classifier
3. nn.Sequential é¡ºåºæ€§ ï¼Œå„ç½‘ç»œå±‚ä¹‹é—´ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œï¼Œå¸¸ç”¨äº block æ„å»º
4. nn.ModuleList è¿­ä»£æ€§ ï¼Œå¸¸ç”¨äºå¤§é‡é‡å¤ç½‘æ„å»ºï¼Œé€šè¿‡ for å¾ªç¯å®ç°é‡å¤æ„å»º
5. nn.ModuleDict ç´¢å¼•æ€§ ï¼Œå¸¸ç”¨äºå¯é€‰æ‹©çš„ç½‘ç»œå±‚
6. AlexNetç‰¹ç‚¹
	1. é‡‡ç”¨ ReLU ï¼šæ›¿æ¢é¥±å’Œæ¿€æ´»å‡½æ•°ï¼Œå‡è½»æ¢¯åº¦æ¶ˆå¤±
	2. é‡‡ç”¨ LRN(Local Response Normalization)ï¼šå¯¹æ•°æ®å½’ä¸€åŒ–ï¼Œå‡è½»æ¢¯åº¦æ¶ˆå¤±
	3. Dropout ï¼šæé«˜å…¨è¿æ¥å±‚çš„é²æ£’æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ, å¢åŠ ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›
	4. Data Augmentation TenCrop ï¼Œè‰²å½©ä¿®æ”¹
	5. 5ä¸ªå·ç§¯å±‚, 3ä¸ªæ± åŒ–å±‚, 3ä¸ªå…¨è¿æ¥å±‚
	6. å·ç§¯åçš„ç»´åº¦: `height = (input - kernel) / stride + 1`
	7. å·ç§¯åçš„ç»´åº¦: `height = (input - dilation * (kernel - 1) + 2 * padding -1 ) / stride + 1`
	8. æ± åŒ–å±‚åçš„ç»´åº¦: `height = (input - pool) / stride + 1`
	9. å·ç§¯å’Œæ± åŒ–éƒ½æ˜¯æ»‘åŠ¨. å·ç§¯æ ¸ï¼šåˆç§°ä¸ºæ»¤æ³¢å™¨ï¼Œè¿‡æ»¤å™¨ã€‚
	10. (224*224*3) ->å·ç§¯(11*11, s=4)-> (54*54*96) ->ReLu-> ->MaxPooling(3*3, s=2)-> (26*26*96)
	11. ->å·ç§¯(5*5, p=2)-> (26*26*256) ->

## Lesson 7: Logistic-Regression-norm.py
1. åœ¨ matplotlib ä¸­ï¼Œä¸€ä¸ªå›¾å½¢ï¼ˆfigureï¼‰å¯ä»¥åŒ…å«å¤šä¸ªå­å›¾ï¼ˆsubplotï¼‰ï¼Œè€Œæ¯ä¸ªå­å›¾åˆåŒ…å«åæ ‡è½´å’Œç»˜å›¾å…ƒç´ ã€‚
2. `plt.clf()` çš„ä½œç”¨æ˜¯æ¸…é™¤å½“å‰å›¾å½¢çš„å†…å®¹ï¼Œå³åˆ é™¤æ‰€æœ‰å­å›¾å’Œåæ ‡è½´ï¼Œä½¿å›¾å½¢å›åˆ°åˆå§‹çŠ¶æ€
3. `mask = y_pred.ge(0.5).float().squeeze()`
4. ä»‹ç»`lr_net.features`å’Œ`å·ç§¯æ ¸`çš„å…³ç³»???
5. æå–æƒé‡å’Œåç½®å‚æ•°, å¯¹è¾“å…¥è¿›è¡Œè®¡ç®—
``` 
# åˆ’åˆ†æ˜¯ä¸€æ¡ç›´çº¿: `y = wx + b`, å’Œ`æƒé‡` `åç½®`å¯¹åº”å…³ç³»ï¼Ÿ

w0, w1 = lr_net.features.weight[0]
w0, w1 = float(w0.item()), float(w1.item())

plot_b = float(lr_net.features.bias[0].item())

plot_x = np.arange(-6, 6, 0.1)
plot_y = (-w0 * plot_x - plot_b) / w1
```

## Lesson 8: transforms_methods_1.py
1. `from torch.utils.data import DataLoader`
2. `import torchvision.transforms as transforms`
3. `sys.path.append(hello_pytorch_DIR)`
4. æŠŠå›¾è±¡å˜æˆå¼ é‡: `transforms.ToTensor()`
5. æŠŠå¼ é‡å˜æˆå›¾è±¡: `def transform_invert(img_, transform_train) -> Image`
6. `img_tensor = inputs[0, ...]`å»æ‰ç¬¬0ç»´çš„æ•°æ®
7. 
```
plt.imshow(img)
plt.show()
plt.pause(0.5)
plt.close()
```

## Lesson 9: my_transforms.py, RMB_data_augmentation.py
1. `Dataset.__getitem__()`ä½¿ç”¨`transform`: `train_data = RMBDataset(data_dir=train_dir, transform=train_transform)`
2. `torch.max(input, dim, keepdim=False, out=None)`: è¿”å›ä¸€ä¸ªå…ƒç»„ (values, indices)ï¼Œå…¶ä¸­ values æ˜¯æ²¿ç€æŒ‡å®šç»´åº¦çš„æœ€å¤§å€¼å¼ é‡ï¼Œindices æ˜¯ç›¸åº”çš„ç´¢å¼•å¼ é‡
3. 
```
valid_transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize(norm_mean, norm_std),
])
```

## Lesson 11: module_containers.py
1. `x.view()` æ˜¯ PyTorch ä¸­ç”¨äºæ”¹å˜å¼ é‡å½¢çŠ¶çš„æ–¹æ³•
2. `x.view( x.size()[0], -1 )`: ç¬¬ä¸€ä¸ªç»´åº¦ä¿æŒä¸å˜ï¼ˆå³ x.size()[0]ï¼‰ï¼Œè€Œç¬¬äºŒä¸ªç»´åº¦è¢«è®¾ç½®ä¸º -1ã€‚å½“æŸä¸ªç»´åº¦çš„å¤§å°è¢«è®¾ç½®ä¸º -1 æ—¶ï¼ŒPyTorch ä¼šæ ¹æ®å¼ é‡çš„æ€»å…ƒç´ æ•°é‡è‡ªåŠ¨è®¡ç®—è¯¥ç»´åº¦çš„å¤§å°ã€‚è¿™ç§åšæ³•å¸¸ç”¨äºå°†å¤šç»´å¼ é‡å±•å¹³ä¸ºäºŒç»´ï¼Œä»¥ä¾¿è¾“å…¥å…¨è¿æ¥å±‚ç­‰æ“ä½œ
3. `nn.ReLU(inplace=True)`
4. featuresç»´åº¦å˜åŒ–: (4, 3, 32, 32) --Conv2d-> (6, 28, 28) --MaxPool2d-> (6, 14, 14) --Conv2d-> (16, 10, 10) --MaxPool2d-> (16, 5, 5)
5. classifierç»´åº¦å˜åŒ–: (16, 5, 5) -> 120 -> 84 -> 2
6. `nn.Sequential()`
7. 
```
nn.Sequential(OrderedDict({
            'conv1': nn.Conv2d(3, 6, 5),
            'relu1': nn.ReLU(inplace=True),
            'pool1': nn.MaxPool2d(kernel_size=2, stride=2),
        }))
```
8. `ModuleDict`æ²¡æœ‰`OrderedDict`, ä¸»è¦ç”¨äºå¤–éƒ¨é€‰æ‹©å¤„ç†
```
nn.ModuleDict({
            'conv': nn.Conv2d(10, 10, 3),
            'pool': nn.MaxPool2d(3)
        })
```
9. `torchvision.models.AlexNet()`
 
## å·ç§¯å±‚: 03-03-ppt-nnç½‘ç»œå±‚-å·ç§¯å±‚.pdf; Code: lesson 12; Video:ã€ç¬¬ä¸‰å‘¨ã€‘nnç½‘ç»œå±‚-å·ç§¯å±‚
1. å·ç§¯æ ¸å­¦ä¹ : è¾¹ç¼˜ï¼Œæ¡çº¹ï¼Œè‰²å½©
2. `nn.Conv2d( in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`
3. åœ¨åŒä¸€æ¬¡å·ç§¯è®¡ç®—ä¸­ï¼Œä¸åŒé€šé“çš„å·ç§¯æ˜¯ç‹¬ç«‹è¿›è¡Œçš„ï¼Œæ¯ä¸ªé€šé“éƒ½æœ‰è‡ªå·±çš„å·ç§¯æ ¸ï¼ˆæƒé‡ï¼‰å’Œå¯¹åº”çš„åç½®ã€‚å…è®¸ç½‘ç»œå­¦ä¹ å¹¶æ•æ‰ä¸åŒç‰¹å¾å±‚æ¬¡çš„ä¿¡æ¯ã€‚
4. `weight`: shape=[2, 3, 3, 3] = [è¾“å‡ºé€šé“æ•°, è¾“å…¥é€šé“æ•°, å·ç§¯æ ¸w, å·ç§¯æ ¸h]; 
5. ä¸ºä»€ä¹ˆä¸‰ç»´å·ç§¯æ ¸å®ç°äºŒç»´å·ç§¯ï¼Ÿæ¯ç»´å·ç§¯æ ¸åªåœ¨ä¸€ä¸ªé€šé“ä¸Šæ»‘åŠ¨ï¼Œä¸‰ç»´å·ç§¯ç»“æœç›¸åŠ ï¼Œå†åŠ ä¸Šåæ‰§ï¼Œå¾—åˆ°ä¸€ä¸ªç»“æœæ•°; 17:21/27:24
6. è½¬ç½®å·ç§¯(Transpose Convolution); éƒ¨åˆ†è·¨è¶Šå·ç§¯ (Fractionally-strided Convolution), ç”¨äºå¯¹å›¾åƒè¿›è¡Œä¸Šé‡‡æ · (UpSample). å·ç§¯æ ¸å½¢çŠ¶ä¸Šæ˜¯è½¬ç½®å…³ç³», ä½†æ˜¯å€¼ä¸ç›¸åŒ
7. å·ç§¯è®¡ç®—è½¬æˆçŸ©é˜µè¿ç®—: éƒ½å˜æˆä¸€ç»´åˆ—å‘é‡ï¼ŒOut = kernel * Input, 18:33
8. `nn.ConvTranspose2d (in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')`
9. è½¬ç½®å·ç§¯å°ºå¯¸è®¡ç®—ï¼šOut=(inâˆ’1) * stride + kernel; å…¬å¼æ˜¯ç›¸é€†çš„
10. è½¬ç½®å·ç§¯å®Œæ•´ç‰ˆï¼šH = (in-1) * stride - 2 * padding + dilation * (kernel-1) + padding + 1
11. è½¬ç½®å·ç§¯stride>1, å®ç°ç‰¹å¾å›¾çš„æ”¾å¤§; æ£‹ç›˜æ•ˆåº”
12. ä½¿ç”¨åœºæ™¯: å·ç§¯ï¼šç”¨äºç‰¹å¾æå–ï¼Œå…·æœ‰å¹³ç§»ä¸å˜æ€§ï¼Œå¸¸è§äºå·ç§¯ç¥ç»ç½‘ç»œçš„å·ç§¯å±‚; è½¬ç½®å·ç§¯ï¼šç”¨äºä¸Šé‡‡æ ·æˆ–åå·ç§¯ï¼Œå¯ä»¥å®ç°ç‰¹å¾å›¾çš„æ”¾å¤§ï¼Œå¸¸è§äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ä¸­

## æ± åŒ–Poolingã€çº¿æ€§Linearã€æ¿€æ´»å‡½æ•°å±‚Activation
1. â€œæ”¶é›†â€ï¼šå¤šå˜å°‘; â€œæ€»ç»“â€ï¼šæœ€å¤§å€¼/å¹³å‡å€¼
2. å·ç§¯å’Œæ± åŒ–éƒ½æ˜¯æ»‘åŠ¨. æ± åŒ–ä¸ºäº†å‡ç»´ï¼Œstride>1
3. ä¸Šé‡‡æ ·æ˜¯ä¸€ç§ç”¨äºå¢åŠ å›¾åƒæˆ–ç‰¹å¾å›¾çš„ç©ºé—´åˆ†è¾¨ç‡çš„æŠ€æœ¯; è½¬ç½®å·ç§¯, åŒçº¿æ€§æ’å€¼, æœ€è¿‘é‚»æ’å€¼; è¯­ä¹‰åˆ†å‰², ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰, ç›®æ ‡æ£€æµ‹, å›¾åƒè¶…åˆ†è¾¨ç‡
4. ä¸‹/é™é‡‡æ ·æ˜¯æŒ‡é™ä½ä¿¡å·æˆ–å›¾åƒçš„é‡‡æ ·ç‡ï¼Œä»è€Œå‡å°æ•°æ®çš„è§„æ¨¡; stride>1å·ç§¯å’Œæ± åŒ–; é™ä½è®¡ç®—æˆæœ¬, å‡å°‘å†…å­˜å ç”¨, æŠ½å–æ›´æŠ½è±¡çš„ç‰¹å¾, é˜²æ­¢è¿‡æ‹Ÿåˆ
5. `nn.MaxPool2d( kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`
6. `nn.AvgPool2d( kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None`
7. `nn.MaxUnpool2d( kernel_size, stride=None, padding=0)`, `forward(self, input , indices, output_size=None)`, 11:59/30:28
8. çº¿æ€§å±‚åˆç§°å…¨è¿æ¥å±‚ï¼Œout = input * W; å’Œå·ç§¯çŸ©é˜µè¿ç®—ç›¸å
9. çº¿æ€§å±‚çš„shape: å…¥åº¦ï¼Œç¥ç»å…ƒçš„ä¸ªæ•°???
10. `nn.Linear( in_features , out_features , bias=True)`, `y = xW^T + bias`
11. nå±‚çº¿æ€§å˜æ¢ç›¸å½“äº1å±‚ï¼Œæ‰€ä»¥å¿…é¡»å¼•å…¥æ¿€æ´»å‡½æ•°(éçº¿æ€§å˜æ¢)ï¼Œèµ‹äºˆæ·±åº¦çš„æ„ä¹‰
12. `nn.Sigmoid`: ç¬¦åˆæ¦‚ç‡; å¯¼æ•°èŒƒå›´æ˜¯ (0, 0.25),æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±; é 0å‡å€¼ï¼Œç ´åæ•°æ®åˆ†å¸ƒ
13. `nn.tanh`: ç¬¦åˆ 0 å‡å€¼; å¯¼æ•°èŒƒå›´æ˜¯ (0, 1), æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
14. `nn.ReLU`: è¾“å‡ºå€¼å‡ä¸ºæ­£æ•°ï¼Œè´ŸåŠè½´å¯¼è‡´æ­»ç¥ç»å…ƒ; å¯¼æ•°æ˜¯ 1, ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œä¸æ˜“å¼•å‘æ¢¯åº¦çˆ†ç‚¸
15. `nn.LeakyReLU`: è´ŸåŠè½´å¾ˆå°æ–œç‡
16. `nn.PReLU`: è´ŸåŠè½´å¯å­¦ä¹ æ–œç‡
17. `nn.RReLU`: è´ŸåŠè½´éšæœºå‡åŒ€åˆ†å¸ƒ
14. gradientæ¢¯åº¦, derivativeè¡ç”Ÿç‰©

## 14. æƒå€¼åˆå§‹åŒ–: 04-01-ppt-æƒå€¼åˆå§‹åŒ–.pdf; ã€ç¬¬å››å‘¨ã€‘æƒå€¼åˆå§‹åŒ–
1. ä¸‹ä¸€å±‚çš„æ¢¯åº¦çš„ä¹˜æ³•å› å­æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡º
2. ç»è¿‡ä¸€å±‚ç½‘ç»œï¼Œè¾“å‡ºæ–¹å·®æ‰©å¤§nå€(nä¸ªç¥ç»å…ƒ)ï¼Œæ ‡å‡†å·®æ‰©å¤§sqrt(n)
3. è§£å†³åŠæ³•: D(w) = 1/n, åªèƒ½æƒå€¼åˆå§‹åŒ–(è€ƒè™‘å‰ï¼åå‘ï¼Œæ¿€æ´»å‡½æ•°)
4. Xavier: (1) æ–¹å·®1, (2) æ¿€æ´»å‡½æ•°: sigmoid, tanh
5. w = [-sqrt(6)/sqrt(n[i] + n[i+1]), sqrt(6)/sqrt(n[i] + n[i+1])]
6. std(w) = 
7. åç§åˆå§‹åŒ–æ–¹æ³•: Xavierå‡åŒ€/æ­£æ€åˆ†å¸ƒ; Kaimingå‡åŒ€/æ­£æ€åˆ†å¸ƒ; å‡åŒ€/æ­£æ€/å¸¸æ•°åˆ†å¸ƒ; æ­£äº¤ï¼å•ä½ï¼ç¨€ç–çŸ©é˜µåˆå§‹åŒ–
8. `nn.init.calculate_gain( nonlinearity, param= None )` è®¡ç®—æ¿€æ´»å‡½æ•°çš„æ–¹å·®å˜åŒ–å°ºåº¦

## 15. æŸå¤±å‡½æ•°1
1. æŸå¤±å‡½æ•°=å•ä¸ªæ ·æœ¬; ä»£ä»·å‡½æ•°=å…¨éƒ¨æ ·æœ¬çš„å¹³å‡; è®¡ç®—ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å€¼
2. ç›®æ ‡å‡½æ•° Obj = Cost + Regularization; (Regularization é˜²æ­¢è¿‡æ‹Ÿåˆ)
3. äº¤å‰ç†µè®¡ç®—`nn.CrossEntropyLoss ( weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')`
4. äº¤å‰ç†µ = ä¿¡æ¯ç†µ + ç›¸å¯¹ç†µ
5. å®ç°è´Ÿå¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸­çš„ è´Ÿå·åŠŸèƒ½`nn.NLLLoss()`
6. äºŒåˆ†ç±»äº¤å‰ç†µ`nn.BCELoss()`
7. ç»“åˆSigmoid ä¸ äºŒåˆ†ç±»äº¤å‰ç†µ`nn.BCEWithLogitsLoss()`, æ¨¡å‹ä¸­ä¸åŠ `Sigmoid`

## 16. æŸå¤±å‡½æ•°2, 04-03-ppt-æŸå¤±å‡½æ•°(äºŒ).pdf, ã€ç¬¬å››å‘¨ã€‘æŸå¤±å‡½æ•°(äºŒ)
1. `nn.CrossEntropyLoss()`
2. `nn.NLLLoss()`
3. `nn.BCELoss()`
4. `nn.BCEWithLogitsLoss`
5. å·®çš„ç»å¯¹å€¼`nn.L1Loss()`
6. å·®çš„å¹³æ–¹`nn.MSELoss()`
7. `nn.SmoothL1Loss()`
8. æ³Šæ¾åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°`nn.PoissonNLLLoss()`
9. KL æ•£åº¦ï¼Œç›¸å¯¹ç†µ`nn.KLDivLoss()`
10. å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦`nn.MarginRankingLoss()`
11. `nn.MultiLabelMarginLoss()`
12. `nn.SoftMarginLoss()`
13. SoftMarginLoss å¤šæ ‡ç­¾ç‰ˆæœ¬`nn.MultiLabelSoftMarginLoss()`
14. è®¡ç®—å¤šåˆ†ç±»çš„æŠ˜é¡µæŸå¤±`nn.MultiMarginLoss()`
15. ä¸‰å…ƒç»„æŸå¤±ï¼Œäººè„¸éªŒè¯ä¸­å¸¸ç”¨`nn.TripletMarginLoss()`
16. ä¸¤ä¸ªè¾“å…¥çš„ç›¸ä¼¼æ€§`nn.HingeEmbeddingLoss`
17. é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ä¸¤ä¸ªè¾“å…¥çš„ç›¸ä¼¼æ€§`nn.CosineEmbeddingLoss()`
18. `nn.CTCLoss()`
19. å‡½æ•°å‚æ•°ä¼ é€’å¯ä»¥åˆ†ä¸¤æ¬¡
```
loss_f_mse = nn.MSELoss(reduction='none')
loss_mse = loss_f_mse(inputs, target)
```

## 17. ä¼˜åŒ–å™¨ Optimizer, 04-04-ppt-ä¼˜åŒ–å™¨ï¼ˆä¸€ï¼‰.pdf
1. `tensor.detach()` åˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œä¸åŸå§‹å¼ é‡å…±äº«ç›¸åŒçš„æ•°æ®ï¼Œä½†ä¸å†è¿½è¸ªæ¢¯åº¦ä¿¡æ¯çš„æ–¹æ³•
2. `Optimizer.state`ï¼šè‡ªå·±çš„å‚æ•°
3. `Optimizer.params_groups`ï¼šç®¡ç†çš„å‚æ•°ç»„
4. `zero_grad()`: æ¸…ç©ºæ‰€ç®¡ç†å‚æ•°çš„æ¢¯åº¦, å¼ é‡æ¢¯åº¦ä¸è‡ªåŠ¨æ¸…é›¶
5. `step()` æ‰§è¡Œä¸€æ­¥æ›´æ–°
6. `add_param_group()` æ·»åŠ å‚æ•°ç»„
7. `state_dict()`ï¼šè·å–ä¼˜åŒ–å™¨å½“å‰çŠ¶æ€ä¿¡æ¯å­—å…¸
8. `torch.save(optimizer.state_dict(), "optimizer_state_dict.pkl")`
8. `load_state_dict()`ï¼šåŠ è½½çŠ¶æ€ä¿¡æ¯å­—å…¸
```
state_dict = torch.load("optimizer_state_dict.pkl")
optimizer.load_state_dict(state_dict)
```

## 18. ä¼˜åŒ–å™¨ Optimizer2, ã€ç¬¬å››å‘¨ã€‘torch.optim.SGD
1. æ¢¯åº¦ä¸‹é™, å­¦ä¹ ç‡æ§åˆ¶æ›´æ–°çš„æ­¥ä¼. w[i+1] = w[i] - LR * g(w[i])
2. å­¦ä¹ ç‡å¤ªå¤§ï¼Œå®¹æ˜“å‘æ•£ã€‚å¤ªå°ï¼Œæ”¶æ•›é€Ÿåº¦å¤ªæ…¢ã€‚å®è·µé€‰ç”¨å°çš„å€¼
3. Momentumï¼ˆåŠ¨é‡ï¼Œå†²é‡ï¼‰ï¼šç»“åˆå½“å‰æ¢¯åº¦ä¸ä¸Šä¸€æ¬¡æ›´æ–°ä¿¡æ¯ï¼Œç”¨äºå½“å‰æ›´æ–°
4. æŒ‡æ•°åŠ æƒå¹³å‡ï¼š`V[t] = B * V[t-1] + (1-B) * O[t]`;  `SUM{(1-B) * B^i * O[n-i]}`
5. Bæ„ä¹‰: è®°å¿†å› å­ã€‚è¶Šå¤§è®°å¿†æ—¶é—´è¶Šé•¿; è¶Šå°è®°å¿†æ—¶é—´è¶ŠçŸ­ã€‚ 
6. Bé€šå¸¸å–0.9ï¼Œ 1/(1-0.9)=10ï¼Œè®°å¿†10å¤©çš„ä¿¡æ¯
7. pytorchä¸­æ›´æ–°å…¬å¼ï¼š`V[i] = M * V[i-1] + g(W[i); W[i+1] = W[i] - LR * V[i]`
8. pytorchä¸­æ›´æ–°å…¬å¼å±•å¼€ï¼šSUM{g(W[i) * M^(n-i)}
9. éšæœºæ¢¯åº¦ä¸‹é™æ³•: `optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)`
10. 10ç§ä¼˜åŒ–å™¨

## 19. 05-01-ppt-å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥.pdf; ã€ç¬¬äº”å‘¨ã€‘å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥
1. `class _LRScheduler`
2. `step()` æ›´æ–°ä¸‹ä¸€ä¸ªepochçš„å­¦ä¹ ç‡
3. `get_lr()` è™šå‡½æ•°
4. ç­‰é—´éš”è°ƒæ•´`lr = lr * gamma`:  `lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)`
5. ç»™å®šé—´éš”è°ƒæ•´`lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)`, `milestones= [50, 125, 160]`
6. æŒ‰æŒ‡æ•°è¡°å‡è°ƒæ•´`lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)`
7. ä½™å¼¦å‘¨æœŸè°ƒæ•´`lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min =0, last_epoch=-1)`, æœ€åæ²¡æœ‰è¡°å‡ï¼Œæ„ä¹‰ï¼Ÿ
8. å½“æŒ‡æ ‡ä¸å†å˜åŒ–åˆ™è°ƒæ•´`lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8)`
9. è‡ªå®šä¹‰è°ƒæ•´ç­–ç•¥`lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)`. ä¾‹å­æœ‰ä¸¤ä¸ª`lr_lambda`ï¼Œå­¦ä¹ ç‡æ€ä¹ˆä½¿ç”¨?
10. æœ‰åºè°ƒæ•´ï¼šStepã€MultiStepã€Exponential å’Œ CosineAnnealing
11. è‡ªé€‚åº”è°ƒæ•´ï¼šReduceLROnPleateau
12. è‡ªå®šä¹‰è°ƒæ•´ï¼šLambda
13. åˆå§‹åŒ–ï¼š(1)è®¾ç½®è¾ƒå°æ•° (2) æœç´¢æœ€å¤§å­¦ä¹ ç‡
14. æ¯ä¸ª`epoch`åªè°ƒæ•´ä¸€æ¬¡ï¼Œåœ¨`iteration`ä¸è°ƒæ•´

## 20. ã€ç¬¬äº”å‘¨ã€‘TensorBoardç®€ä»‹ä¸å®‰è£…; 05-02-ppt-TensorBoardç®€ä»‹ä¸å®‰è£….pdf
1. `pip install tensorboard`. If No module named 'past', `pip install future`
2. å­å›¾: main_tag, tag_scalar_dict
```
writer = SummaryWriter(comment='test_tensorboard')
writer.add_scalar('y=2x', x * 2, x)			# tag, Y, X
writer.add_scalars()
writer.close()

tensorboard --logdir=./runs
http://localhost:6006/
```

## 21. ã€ç¬¬äº”å‘¨ã€‘TensorBoardä½¿ç”¨ï¼ˆä¸€ï¼‰; 05-03-ppt-TensorBoardä½¿ç”¨ï¼ˆä¸€ï¼‰.pdf
1. `class SummaryWriter`
2. `add_scalar(tag, scalar_value, global_step =None, walltime=None)`, global_step=X
3. `add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)`, `tag_scalar_dict: keyæ˜¯å˜é‡çš„tag, valueæ˜¯å˜é‡çš„å€¼`
4. `add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None)`

## 22. ã€ç¬¬äº”å‘¨ã€‘TensorBoardä½¿ç”¨ï¼ˆäºŒï¼‰; 05-04-ppt-TensorBoardä½¿ç”¨ï¼ˆäºŒï¼‰.pdf
1. `import torchvision.utils as vutils`, `from torch.utils.tensorboard import SummaryWriter`
2. å›¾ç‰‡é‡å  `add_image(tag, img_tensor, global_step =None, walltime=None, dataformats ='CHW')`
3. å›¾ç‰‡ç½‘æ ¼ `make_grid(tensor, nrow =8, padding=2, normalize=False, range=None, scale_each =False, pad_value=0)`, `add_image()`
4. æ¨¡å‹è®¡ç®—å›¾ `add_graph( model, input_to_model=None, verbose=False)`
5. æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯ `summary(model, input_size, batch_size=-1, device="cuda")`
6. `if isinstance(sub_module, nn.Conv2d)`
7. å·ç§¯æ ¸çš„å‚æ•°åŒ…æ‹¬æƒé‡(Weights), åç½®(Bias). å·ç§¯æ ¸çš„æƒé‡æ˜¯å…±äº«çš„
8. `weights.shape`å½¢çŠ¶é€šå¸¸è¡¨ç¤ºä¸º `[out_channels, in_channels, kernel_height, kernel_width]`, out_channelsï¼ˆè¾“å‡ºé€šé“æ•°ï¼‰ï¼šè¿™æ˜¯å·ç§¯å±‚ä¸­å·ç§¯æ ¸çš„æ•°é‡ï¼Œä¹Ÿæ˜¯è¯¥å±‚è¾“å‡ºçš„ç‰¹å¾å›¾çš„é€šé“æ•°ã€‚æ¯ä¸ªå·ç§¯æ ¸å¯¹è¾“å…¥ä¸‰ä¸ªé€šé“è®¡ç®—ç»“æœç›¸åŠ ä¼šç”Ÿæˆä¸€ä¸ªè¾“å‡ºé€šé“ã€‚
9. å…¨è¿æ¥å±‚ä¸­, æ¯ä¸ªè¿æ¥éƒ½æœ‰ä¸€ä¸ªæƒé‡å‚æ•°
10. `sub_module.weight`
11. `sub_module.bias`, æ¯ä¸ªè¾“å‡ºé€šé“éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„åç½®
12. `img_pil = PIL.Image.open(path_img)`, `print(img_pil.mode)`, RGB
13. å‚æ•°è®¡ç®—: å·ç§¯æ ¸`[6, 3, 5, 5]`(ä¸€ä¸ªå·ç§¯æ ¸åœ¨æ¯ä¸ªé€šé“éƒ½æœ‰ä¸åŒå‚æ•°), ä¹˜ç§¯åŠ ä¸Š6ä¸ªbias = 456

## 23. ã€ç¬¬äº”å‘¨ã€‘hookå‡½æ•°ä¸CAMå¯è§†åŒ–; 05-05-ppt-hookå‡½æ•°ä¸CAMå¯è§†åŒ–.pdf
1. ç›®çš„: ä¸æ”¹å˜ä¸»ä½“ï¼Œæå–ç‰¹å¾å›¾/æ”¹å˜æ¢¯åº¦
2. tensoråå‘ä¼ æ’­ `torch.Tensor.register_hook (hook)`
3. æ¨¡å—é’©å­ `torch.nn.Module.register_forward_hook()`
4. `torch.nn.Module.register_forward_pre_hook()`
5. `torch.nn.Module.register_backward_hook()`
6. `handle.remove()`
7. `feature map`å·ç§¯å±‚çš„è¾“å‡ºï¼Œæˆ–è¾“å‡ºé€šé“çš„é›†åˆ. æ¯ä¸ªç‰¹å¾å›¾å¯¹åº”ä¸€ä¸ªå·ç§¯æ ¸. å½¢çŠ¶[BCHW], Cå·ç§¯æ ¸çš„æ•°é‡
8. å·ç§¯å±‚çš„æƒé‡ï¼ˆweightï¼‰æ˜¯æŒ‡å·ç§¯æ ¸çš„å‚æ•°, å½¢çŠ¶[out_channels, in_channels, kernel_height, kernel_width]
9. out_channels æ˜¯å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ï¼Œè¡¨ç¤ºå·ç§¯æ ¸çš„æ•°é‡ã€‚æ¯ä¸ªå·ç§¯æ ¸ç”Ÿæˆä¸€ä¸ªè¾“å‡ºé€šé“ï¼Œè®¡ç®—ç»“æœæ˜¯å·ç§¯å±‚çš„ä¸€ä¸ªç‰¹å¾å›¾
10. è¾“å‡ºé€šé“æ•° : å·ç§¯æ ¸çš„æ•°é‡ : ç‰¹å¾å›¾ = 1 : 1 : 1
11. è¾“å…¥é€šé“æ•° : æ¯ä¸ªå·ç§¯æ ¸çš„å‚æ•°æ•°é‡ = 1 : 1
12. CAMï¼šç±»æ¿€æ´»å›¾ class activation map. æŠŠæœ€åè¾“å‡ºçš„ç‰¹å¾å›¾å’Œæƒå€¼ç›¸ä¹˜å–å’Œï¼Œçœ‹ç®—æ³•æ ¹æ®å“ªäº›éƒ¨åˆ†å¾—åˆ°åˆ†ç±»ç»“æœ
13. å…¨å±€æ± åŒ–ï¼ˆGlobal Poolingï¼‰å¯¹äºæ¯ä¸ªé€šé“å–å€¼(æ˜¯ä¸€ä¸ªæ ‡é‡)ï¼Œæœ€åæŠŠç»“æœä¸²è”ï¼Œä»è€Œå°†æ•´ä¸ªç‰¹å¾å›¾è½¬åŒ–ä¸ºä¸€ä¸ªä¸€ç»´æ•°ç»„
14. ç‰¹å¾å›¾çš„é€šé“æ•° : å…¨å±€æ± åŒ–ä¸€ç»´æ•°ç»„çš„é•¿åº¦ = 1 : 1
15. æ™®é€šçš„æ± åŒ–ï¼Œç‰¹å¾å›¾çš„ä¸€ä¸ªé€šé“æ˜¯äºŒç»´ï¼Œæ± åŒ–åè¿˜æ˜¯äºŒç»´ï¼Œåªæ˜¯é™ç»´
16. CAMï¼šç¼ºç‚¹éœ€è¦æ”¹å˜ç½‘ç»œç»“æ„ï¼Œé‡æ–°è®­ç»ƒ. (åªèƒ½è®­ç»ƒå®Œï¼Œçœ‹çœ‹è®­ç»ƒçš„å…³æ³¨ç‚¹å¯¹å¦ï¼Ÿ)
17. Grad-CAM: æ”¹è¿›ç‰ˆï¼Œåˆ©ç”¨æ¢¯åº¦ä½œä¸ºç‰¹å¾å›¾æƒé‡. é€šè¿‡é’©å­å‡½æ•°ï¼Œä¸ç”¨ä¿®æ”¹æ¨¡å‹ï¼Œä¸ç”¨é‡è®­ç»ƒ
18. [PyTorchçš„hookåŠå…¶åœ¨Grad-CAMä¸­çš„åº”ç”¨](https://zhuanlan.zhihu.com/p/75894080)
19. ```
fmap_dict = dict()
fmap_dict.setdefault(key_name, list())

fmap_dict[key_name].append(o)

alexnet._modules[n1]._modules[n2].register_forward_hook(hook_func)
```

## 24. ã€ç¬¬å…­å‘¨ã€‘æ­£åˆ™åŒ–ä¹‹weight_decay;	06-01-ppt-æ­£åˆ™åŒ–ä¹‹weight_decay.pdf
1. Regularizationï¼šå‡å°æ–¹å·®
2. è¯¯å·® = åå·® + æ–¹å·® + å™ªå£°
3. åå·®åº¦é‡äº†å­¦ä¹ ç®—æ³•çš„æœŸæœ›é¢„æµ‹ä¸çœŸå®ç»“æœçš„åç¦»ç¨‹åº¦ï¼Œå³åˆ»ç”»äº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ›
4. æ–¹å·®åº¦é‡äº†åŒæ ·å¤§å°çš„è®­ç»ƒé›†çš„å˜åŠ¨æ‰€å¯¼è‡´çš„å­¦ä¹ æ€§èƒ½çš„å˜åŒ–ï¼Œå³åˆ»ç”»äº†æ•°æ®æ‰°åŠ¨æ‰€é€ æˆçš„å½±å“
5. å™ªå£°åˆ™è¡¨è¾¾äº†åœ¨å½“å‰ä»»åŠ¡ä¸Šä»»ä½•å­¦ä¹ ç®—æ³•æ‰€èƒ½è¾¾åˆ°çš„æœŸæœ›æ³›åŒ–è¯¯å·®çš„ä¸‹ç•Œ
6. è¿‡æ‹Ÿåˆå°±æ˜¯æ–¹å·®å¤§
7. `L1`å‚æ•°çš„ç»å¯¹å€¼ä¹‹å’Œ, ç›´çº¿ -> è±å½¢
8. åŠ ä¸Š`L1`çš„è§£åœ¨åæ ‡è½´ä¸Šï¼Œå¦ä¸€ä¸ªå‚æ•°ä¸º0ï¼Œç¨€ç–è§£
9. `L2`å‚æ•°å¹³æ–¹å’Œï¼Œåœ†å½¢, weight decay( æƒå€¼è¡°å‡ )
10. `nn.Module.named_modules()` æ–¹æ³•è¿”å›æ¨¡å—åŠå…¶å­æ¨¡å—çš„è¿­ä»£å™¨ã€‚æ¯ä¸ªæ¨¡å—éƒ½æœ‰ä¸€ä¸ª`data`å±æ€§å’Œä¸€ä¸ª `parameters()`æ–¹æ³•ï¼Œå®ƒä»¬åˆ†åˆ«ç”¨äºè·å–æ¨¡å—çš„éå‚æ•°æ•°æ®å’Œå¯å­¦ä¹ å‚æ•°
11. `for name, layer in nn.Modulenamed.named_parameters():`: `nn.Module.named_parameters()` è¿”å›æ¨¡å—ä¸­æ‰€æœ‰å¯å­¦ä¹ å‚æ•°çš„è¿­ä»£å™¨. å€¼æœ‰:`linears.0.weight`, `linears.0.bias`, æ¯ä¸ªå†åˆ†ä¸¤ä¸ª:
12. `layer.data` å‚æ•°çš„å®é™…å€¼
13. `layer.grad` å‚æ•°çš„æ¢¯åº¦

## 25. ã€ç¬¬å…­å‘¨ã€‘æ­£åˆ™åŒ–ä¹‹Dropout; 06-02-ppt-æ­£åˆ™åŒ–-Dropout.pdf
1. Dropoutï¼šç¥ç»å…ƒéšæœºå¤±æ´», è·Ÿå®ƒç›¸å…³çš„weight = 0, é˜²æ­¢å¯¹æŸä¸€ä¸ªç¥ç»å…ƒè¿‡åº¦ä¾èµ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
2. ç”±äºè®­ç»ƒæ—¶å¤±æ´»ï¼Œæ•°æ®å¤§å°ç¼©å°ï¼Œæ‰€ä»¥æµ‹è¯•æ—¶ä¹Ÿè¦ç¼©å°, w *= (1-p)ã€‚`torch`ä¸ºäº†é¿å…ç¼©å°æµ‹è¯•æ•°æ®ï¼Œåœ¨è®­ç»ƒæ—¶æ”¾å¤§è®­ç»ƒæ•°æ®
3. `torch.nn.Dropout(p=0.5, inplace =False)`
4. æ”¾åœ¨éœ€è¦dropoutå±‚çš„å‰é¢ï¼Œä½†æ˜¯åœ¨æœ€åè¾“å‡ºå±‚ä¸€èˆ¬ä¸åŠ 
5. ç±»ä¼¼`L2`æ”¶ç¼©æ•°æ®å°ºåº¦ï¼Œå‡å°‘æƒé‡çš„æ–¹å·®
6. å¼€å§‹è®­ç»ƒæ¨¡å¼`net.train()`; å¼€å§‹æµ‹è¯•æ¨¡å¼`net.eval()`

## 26. ã€ç¬¬å…­å‘¨ã€‘Batch Normalization; 06-03-Batch Normalization.pdf
1. Batch Normalization ï¼šæ‰¹æ ‡å‡†åŒ–, 0 å‡å€¼ï¼Œ 1 æ–¹å·®
2. ä¼˜ç‚¹ï¼š
	1.å¯ä»¥ç”¨æ›´å¤§å­¦ä¹ ç‡ï¼ŒåŠ é€Ÿæ¨¡å‹æ”¶æ•›
	2.å¯ä»¥ä¸ç”¨ç²¾å¿ƒè®¾è®¡æƒå€¼åˆå§‹åŒ–
	3.å¯ä»¥ä¸ç”¨dropoutæˆ–è¾ƒå°çš„dropout
	4.å¯ä»¥ä¸ç”¨L2æˆ–è€…è¾ƒå°çš„ weight decay
	5.å¯ä»¥ä¸ç”¨LRN(local response normalization)
3. `Internal Covariate shift (ICS)` æ•°æ®å°ºåº¦åˆ†å¸ƒå˜åŒ–
	1. æ±‚å‡å€¼
	2. æ±‚æ ‡å‡†å·®
	3. å½’ä¸€åŒ–
	4. å¯å­¦ä¹ çš„é€†å½’ä¸€åŒ–: ä»¿å°„å˜æ¢=æ”¾ç¼©+å¹³ç§», (r, B) = (weight, bias)
4. æœ€åå±‚çš„æ•°æ®å°ºåº¦ = æ¯ä¸€å±‚æ•°æ®å°ºåº¦ ç›¸ä¹˜
5. å¿…é¡»åœ¨`relu`å‰ä½¿ç”¨BN, å› ä¸º`relu`ä¼šæ”¹å˜æ•°æ®åˆ†å¸ƒ
6. ```
    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):				# biaséƒ½æ˜¯æ¸…0
                nn.init.xavier_normal_(m.weight.data)	# å·ç§¯å±‚åˆå§‹åŒ–: å·ç§¯æ ¸
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):			# BNå±‚åˆå§‹åŒ–: å‚æ•°ç½®1
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):				# çº¿æ€§è¿æ¥å±‚: å‚æ•°æ ‡å‡†æ­£æ€åˆ†å¸ƒ
                nn.init.normal_(m.weight.data, 0, 1)
                m.bias.data.zero_()
```
7. åŸºç±» `_BatchNorm__(num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True)`
8. æ´¾ç”Ÿç±» `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.BatchNorm3d`
9. `num_features`ï¼šä¸€ä¸ªæ ·æœ¬ç‰¹å¾æ•°é‡(æœ€é‡è¦). `affine`æ˜¯å¦å­¦ä¹ 3.4(r, B), `track_running_stats`ï¼šæ˜¯è®­ç»ƒçŠ¶æ€ï¼Œè¿˜æ˜¯æµ‹è¯•çŠ¶æ€
10. running_meanï¼šç»è¿‡åŠ¨é‡è®¡ç®—åå‡å€¼; running_varï¼šåŠ¨é‡åæ–¹å·®; weight: affine transform ä¸­çš„ gamma; bias affine transform ä¸­çš„ beta
11. å·ç§¯åçš„ç‰¹å¾æ•°é‡ = é€šé“æ•° = å·ç§¯æ ¸çš„æ•°é‡ 
12. input= B * ç‰¹å¾æ•° * 1/2/3dç‰¹å¾
13. 4ä¸ª`shape`ç›¸åŒ(=å·ç§¯æ ¸çš„æ•°é‡): running_mean, running_var, weight, bias; æ‰€ä»¥BNå±‚å¯ä»¥éšä¾¿æ’å…¥æˆ–ç§»å‡º
14. PPTä¸­input.shape: (1)=3*5*1; (2)=3*3*2*2; (3)=3*4*2*2*3
15. æ±‚å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨åŒä¸€ä¸ªç‰¹å¾ä¸Šï¼Œå¯¹batch sizeä¸ªæ•°æ®æ±‚å‡ºä¸€ä¸ªæ ‡é‡

## 27. ã€ç¬¬å…­å‘¨ã€‘Normalizaiton_layers; 06-04-ppt-Normalizaiton_layers.pdf
1. Internal Covariate Shift (ICS)ï¼šæ•°æ®å°ºåº¦/åˆ†å¸ƒå¼‚å¸¸ï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾
2. `ğƒ(HğŸ)=ğ’âˆ—ğ‘«(ğ‘¿)âˆ—ğ‘«(ğ‘¾)`; å‚æ•°çº¬åº¦æ˜¯n, xæ˜¯è¾“å…¥ï¼Œwæ˜¯å‚æ•°; æ¢¯åº¦çš„æ¶ˆå¤±ï¼çˆ†ç‚¸
3. æ¦‚æ‹¬: å‡å‡å€¼ï¼Œ é™¤æ ‡å‡†å·®ï¼Œä¹˜rï¼ŒåŠ B
4. å…¶å®ƒçš„æ­£åˆ™åŒ–, å·®å¼‚: å‡å€¼å’Œæ–¹å·®æ±‚å–æ–¹å¼
	1. Batch Normalization: æ±‚å‡å€¼å’Œæ–¹å·®åœ¨ ä¸€è¡Œ
	2. Layer Normalization: æ±‚å‡å€¼å’Œæ–¹å·®åœ¨ ä¸€åˆ—
	3. Instance Normalization: ä¸€åˆ—ä¸­æ¯ä¸ªé€šé“å•ç‹¬è®¡ç®—
	4. Group Normalization: ä¸€åˆ—ä¸­å¤šä¸ªé€šé“æˆç»„è®¡ç®—
5. LN: BNä¸é€‚ç”¨äºå˜é•¿çš„ç½‘ç»œï¼Œé€å±‚è®¡ç®—å‡å€¼å’Œæ–¹å·®, ä¸å†æœ‰running_meanå’Œrunning_var, gamma å’Œ beta ä¸ºé€å…ƒç´ çš„
6. `nn.LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True)`
7. ln.weight.shape
	1. elementwise_affine=True, å’Œè¾“å…¥çš„shapeä¸€æ ·
	2. elementwise_affine=False, æ ‡é‡
	3. å¯ä»¥ä»è¾“å…¥çš„shapeåé¢å‘å‰å–ä¸€éƒ¨åˆ†
8. IN: BN åœ¨å›¾åƒç”Ÿæˆï¼ˆ Image Generation ï¼‰(é£æ ¼è¿ç§»)ä¸­ä¸é€‚ç”¨; é€ Instance channel è®¡ç®—å‡å€¼å’Œæ–¹å·®
9. `nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)`
10. GN: åº”ç”¨åœºæ™¯ï¼šå¤§æ¨¡å‹ï¼ˆç‰¹å¾å›¾å¤š, å°batch size ï¼‰; BN ä¼°è®¡çš„å€¼ä¸å‡†; é€šé“æ¥å‡‘; ä¸å†æœ‰ running_mean å’Œ running_var; gamma å’Œ beta ä¸ºé€é€šé“ï¼ˆ channel ï¼‰
11. `nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)`, é€šé“èƒ½è¢«ç»„æ•°æ•´é™¤
12. gn.weight.shape = è¾“å…¥é€šé“æ•°

## 28. ã€ç¬¬ä¸ƒå‘¨ã€‘æ¨¡å‹ä¿å­˜ä¸åŠ è½½; 07-01-ppt-æ¨¡å‹ä¿å­˜ä¸åŠ è½½.pdf
1. `torch.save()`; ä¿å­˜æ•´ä¸ª Module, ä¿å­˜æ¨¡å‹å‚æ•°; `torch.save(net, path_model)`, `torch.save(net.state_dict(), path_state_dict)`
2. `torch.load()`; `net_load = torch.load(path_model)`,  `net_new.load_state_dict(torch.load(path_state_dict))`
3. æ–­ç‚¹ç»­è®­ç»ƒ: ```
checkpoint = {
	"model_state_dict ": net.state_dict
	"optimizer_state_dict": optimizer.state_dict
	"epoch": epoch
}

torch.save(checkpoint, path_checkpoint)

checkpoint = torch.load(path_checkpoint)
net.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch']
scheduler.last_epoch = start_epoch

for epoch in range(start_epoch + 1, MAX_EPOCH):
```

## 29. ã€ç¬¬ä¸ƒå‘¨ã€‘æ¨¡å‹finetune; 07-02-ppt-æ¨¡å‹finetune.pdf
1. TransferLearning ï¼šè¿ç§»
2. Model Finetune: æ¨¡å‹å¾®è°ƒ; å‚æ•°=çŸ¥è¯†; features extractorä¸å˜ï¼Œclassifieræ”¹å˜
3. é‡æ–°è®­ç»ƒè€—æ—¶é•¿; è€Œä¸”æ–°çš„æ•°æ®é›†ä¸€èˆ¬æ¯”è¾ƒå°ï¼Œæ•ˆæœä¸å¥½
4. æ¨¡å‹å¾®è°ƒæ­¥éª¤ï¼š
	1. è·å–é¢„è®­ç»ƒæ¨¡å‹å‚æ•°
	2. åŠ è½½æ¨¡å‹: load_state_dict(), å…¨é“¾æ¥å±‚å‚æ•°æ˜¯å¦ä¹ŸåŠ è½½ï¼Ÿ
	3. ä¿®æ”¹è¾“å‡ºå±‚
5. æ¨¡å‹å¾®è°ƒè®­ç»ƒæ–¹æ³•ï¼š
	1. å›ºå®šé¢„è®­ç»ƒçš„å‚æ•°: requires_grad =False æˆ– lr =0
	2. Features Extractor è¾ƒå°å­¦ä¹ ç‡ ( params_group )
6. Resnet-18: æ•°æ® https://download.pytorch.org/tutorial/hymenoptera_data.zip
7. æ¨¡å‹ https://download.pytorch.org/models/resnet18-5c106cde.pth
8. åŠ è½½å‚æ•°
9. æ›¿æ¢fcå±‚, å…ˆå¾—å‡ºè¾“å…¥é€šé“: `num_ftrs = resnet18_ft.fc.in_features      resnet18_ft.fc = nn.Linear(num_ftrs, classes)`
10. å†»ç»“å·ç§¯å±‚å› ä¸ºæ–°æ•°æ®é‡å¾ˆå°ï¼Œä¸ºäº†è¡¥è¶³FCå‚æ•°(æ–°å¢åŠ )ï¼Œéœ€è¦è®­ç»ƒå¤šæ¬¡ï¼Œå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆ`for param in resnet18_ft.parameters(): param.requires_grad = False`
11. conv å°å­¦ä¹ ç‡: ```
fc_params_id = list(map(id, resnet18_ft.fc.parameters()))     # è¿”å›çš„æ˜¯fc parametersçš„ å†…å­˜åœ°å€
base_params = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())
optimizer = optim.SGD([
    {'params': base_params, 'lr': LR*0},   				# æ€ä¹ˆçŸ¥é“ä¸¤ä¸ªå‚æ•°ç»„ï¼Œä¸€ä¸ªä¸ºå·ç§¯ï¼Œä¸€ä¸ªä¸ºFC
    {'params': resnet18_ft.fc.parameters(), 'lr': LR}], momentum=0.9)
```
12. GPU ```
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet18_ft.to(device)									# æ¨¡å‹é€åˆ°GPUä¸Š
inputs, labels = inputs.to(device), labels.to(device)	# æ•°æ®é€åˆ°GPUä¸Š
```

## 30 ã€ç¬¬ä¸ƒå‘¨ã€‘GPUçš„ä½¿ç”¨; 07-03-ppt-GPUçš„ä½¿ç”¨.pdf
1. CPU æ§åˆ¶å•å…ƒå¤šï¼ŒGPU è¿ç®—å•å…ƒå¤š
2. è½¬æ¢æ•°æ®ç±»å‹ /è®¾å¤‡: `data.to("cuda")`, `model.to(torch.device("cpu"))`; å¼ é‡ä¸æ‰§è¡Œinplaceï¼Œæ¨¡å‹æ‰§è¡Œinplace
3. æ¨¡å‹è½¬åˆ°`GPU`ä¸Šï¼Œæ¨¡å‹æ‰§è¡Œinplace, ä½†æ˜¯åœ°å€ä¸ºä»€ä¹ˆä¸å˜? å› ä¸º inplace æ“ä½œä¿®æ”¹äº† GPU ä¸Šçš„å­˜å‚¨ï¼Œè€Œæ¨¡å‹æœ¬èº«çš„ç»“æ„ä»ç„¶å­˜å‚¨åœ¨ CPU å†…å­˜ä¸­
4. é€»è¾‘gpu(pyå¯è§) <= ç‰©ç†gpu
5. å¸¸ç”¨æ–¹æ³• ```
torch.cuda.device_count()
torch.cuda.get_device_name()
torch.cuda.manual_seed_all()
os.environ.setdefault ("CUDA_VISIBLE_DEVICES", "2,3")
```
6. ä¸»GPU(0): åˆ†å‘ -> å¹¶è¡Œè¿ç®— -> ç»“æœå›æ”¶. åœ¨ä¸€ä¸ªbatchä¸­åˆ†å‘
7. `torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)`
8. GPUå†…å­˜æ’åº ```
def get_gpu_memory():
	import platform
    if 'Windows' != platform.system():
		import os
		os.system('nvidia-smi -q -d Memory | grep -A4 GPU | grep Free > tmp.txt')
		memory_gpu = [int(x.split()[2]) for x in open('tmp.txt', 'r').readlines()]
		os.system('rm tmp.txt')
		return memory_gpu
	else:
        print("æ˜¾å­˜è®¡ç®—åŠŸèƒ½æš‚ä¸æ”¯æŒwindowsæ“ä½œç³»ç»Ÿ")
		return False
			
gpu_memory = get_gpu_memory()
if gpu_memory:
	gpu_list = np.argsort(gpu_memory)[::-1]
	gpu_list_str = ','.join( map(str, gpu_list))
	os.environ.setdefault("CUDA_VISIBLE_DEVICES", gpu_list_str)

	print("\ngpu free memory: {}".format( gpu_memory ))
	print("CUDA_VISIBLE_DEVICES :{}".format(os.environ ["CUDA_VISIBLE_DEVICES"]))
```
9. æ¨¡å‹å‚æ•°å­˜æ˜¯å¦åœ¨gpuä¸Š, `torch.load( path_state_dict , map_location= "cpu")`
10. å¤šGPUè®­ç»ƒï¼Œå‚æ•°å‰é¢å¤šäº†'module.' ```
from collections import OrderedDict
new_state_dict = OrderedDict
for k, v in state_dict_load.items():
	namekey= k[7:] if k.startswith ('module.') else k
	new_state_dict[namekey] = v
```

## 31 ã€ç¬¬ä¸ƒå‘¨ã€‘PyTorchå¸¸è§æŠ¥é”™; 07-04-ppt-PyTorchå¸¸è§æŠ¥é”™.pdf
1. [å¸¸è§æŠ¥é”™](https://shimo.im/docs/PvgHytYygPVGJ8Hv/)
2. `dataparallel`ï¼Œæ‰€æœ‰ module éƒ½å¢åŠ ä¸€ä¸ªå±æ€§ `module`
3. åŠ è½½ä¿å­˜çš„ç½‘ç»œæ¨¡å‹å‰, å¿…é¡»å®šä¹‰ç½‘ç»œæ¨¡å‹
4. æ ‡ç­¾ä» 0 å¼€å§‹, è€Œä¸æ˜¯ä» 1 å¼€å§‹
5. æŸ¥çœ‹æ•°æ®åœ¨å“ªé‡Œ`data.device`

## 32 ã€ç¬¬å…«å‘¨ã€‘å›¾åƒåˆ†ç±»ä¸€ç¥; 08-01-ppt-å›¾åƒåˆ†ç±»ä¸€ç¥.pdf
1. Inference æ¨ç†. (1)`resnet18.eval()`, (2)`with torch.no_grad():`, (3)æ•°æ®é¢„å¤„ç†éœ€ä¿æŒä¸€è‡´ï¼Œ
2. æ¨¡å‹åº“`anaconda3\envs\pytorchTest\Lib\site-packages\torchvision\models`
3. ç»å…¸æ¨¡å‹: alexnet, densenet, googlenet, inception, resnet, vgg, mnasnet
4. è½»é‡æ¨¡å‹: mobilenet, shufflenetv2, squeezenet
5. ResNet: åŸå§‹Xå‰å‘ä¼ æ’­ï¼Œå‡è½»æ¢¯åº¦æ¶ˆå¤±ï¼Œå¢åŠ ç½‘ç»œå±‚æ¬¡, BasicBlock
6. ä¸€å¼€å§‹è¾“å…¥å›¾è±¡: 224 * 224; å¤„ç†æµç¨‹`forward(self, x: Tensor)`
7. 3 * 3, 64: å·ç§¯æ ¸æ˜¯3*3ï¼Œå…±64ä¸ªæ ¸

## 33 ã€ç¬¬å…«å‘¨ã€‘å›¾åƒåˆ†å‰²ä¸€ç¥; 08-02-ppt-å›¾åƒåˆ†å‰²ä¸€ç¥.pdf
1. å›¾åƒåˆ†å‰²(Image Segmentation)ï¼šå°†å›¾åƒæ¯ä¸€ä¸ªåƒç´ åˆ†ç±». å›¾åƒåˆ†ç±»ï¼šå°†å›¾åƒç»™ä¸€ä¸ªåˆ†ç±»
2. å›¾åƒåˆ†å‰²åˆ†ç±»ï¼šè¶…åƒç´ åˆ†å‰²(é¢„å¤„ç†), è¯­ä¹‰åˆ†å‰²(é»˜è®¤), å®ä¾‹åˆ†å‰²(æ²¡æœ‰èƒŒæ™¯), å…¨æ™¯åˆ†å‰²(è¯­ä¹‰åˆ†å‰² + å®ä¾‹åˆ†å‰²)
3. (C, W, H) -> (åˆ†ç±»çš„ç±»åˆ«, W, H)
4. `model = torch.hub.load( github , model, *args , **kwargs)`
5. `torch.hub.list( github , force_reload =False)`
6. `torch.hub.help( github , model, force_reload =False)`
7. å¾—åˆ°é¢„è®­ç»ƒçš„æ¨¡å‹`https://pytorch.org/hub/`
8. åƒç´ é—´ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€ƒè™‘äº†å…³é”®éƒ¨ä½
9. FCN: å…¨å·ç§¯, å»æ‰äº†å…¨é“¾æ¥å±‚ï¼Œè¾“å…¥å›¾è±¡å¤§å°å¯å˜
10. UNet: 1*572*572 -> 2*388*388, å±‚é—´æ‹·è´
11. DeepLab V1: (1)å­”æ´å·ç§¯ï¼Œå¢å¤§æ„Ÿå—é‡ (2) é‡‡ç”¨ CRFæ¡ä»¶éšæœºåœº(Conditional Random Field) è¿›è¡Œ mask åå¤„ç†
12. DeepLab V2: å·ç§¯æ ¸å¤§å°ä¸å˜ï¼Œä½†strikeå˜åŒ–ã€‚å·ç§¯ç©ºæ´ç©ºé—´é‡‘å­—å¡”æ± ASPP(Atrous spatial pyramid pooling ï¼‰ï¼šè§£å†³å¤šå°ºåº¦é—®é¢˜
13. DeepLab V3: (1) å­”æ´å·ç§¯çš„ä¸²è¡Œ (2) ASPP çš„å¹¶è¡Œ
	1. Image Pyramid: å¤šä¸ªæ„Ÿå—é‡/strikeçš„ç»“æœï¼Œæœ€åèåˆ
	2. Encoder-Decoder: UNet
	3. Deeper w. Atrous Convolution: ç»è¿‡ç©ºæ´å·ç§¯åï¼Œå›¾è±¡åˆ†è¾¨ç‡ç¼©å°å˜æ…¢
	4. Spatial Pyramid Pooling:
14. DeepLab V3+: deeplabv3 åŸºç¡€ä¸ŠåŠ ä¸Š Encoder Decoder æ€æƒ³
15. ç»¼è¿°: ã€ŠDeep Semantic Segmentation of Natural and Medical Images: A Review ã€‹ 2019
16. å®ç°äººåƒæŠ å›¾ (Portrait Matting)

## 34 ã€ç¬¬å…«å‘¨ã€‘å›¾åƒç›®æ ‡æ£€æµ‹ä¸€ç¥ï¼ˆä¸Šï¼‰ï¼ˆä¸‹ï¼‰; 08-03-ppt-å›¾åƒç›®æ ‡æ£€æµ‹ä¸€ç¥.pdf
1. èƒŒæ™¯æ˜¯p0
2. ä¸¤è¦ç´ /è¾“å‡º: (1) åˆ†ç±»; (2)å›å½’è¾¹ç•Œæ¡†
3. è¯†åˆ«å‡ºæ¥ç‰©ä½“æ•°é‡çš„ç¡®å®š:
	1. ä¼ ç»Ÿæ–¹æ³•: æ»‘åŠ¨çª—; ç¼ºç‚¹ï¼š(1) é‡å¤è®¡ç®—é‡å¤§, (2) çª—å£å¤§å°éš¾ç¡®å®š
	2. åˆ©ç”¨å·ç§¯å‡å°‘é‡å¤è®¡ç®—: ç‰¹å¾å›¾ä¸€ä¸ªåƒç´  å¯¹åº” åŸå›¾ ä¸€å—åŒºåŸŸ
4. ä¸¤ä¸ªé˜¶æ®µå¤šå‡ºä¸€ä¸ªæ¨èæ¡†Proposal Rect; ä¸€é˜¶æ®µç›´æ¥æŠŠç‰¹å¾å›¾åˆ†æˆN*Nç½‘æ ¼
5. ä¸€ä¸ªé˜¶æ®µç®—æ³•: YOHO, SSD, Retina-Net, 
6. ä¸¤ä¸ªé˜¶æ®µç®—æ³•: RCNN, Fast RCNN, Pyramid Network. åŒºåŸŸå»ºè®® + å¯¹è±¡æ£€æµ‹
7. Faster RCNN æ•°æ®æµåˆ†å››ä¸ªé˜¶æ®µ:
	1. Feature map
	2. RPNåŒºåŸŸæè®®ç½‘ç»œï¼ˆRegion Proposal Networkï¼‰: (1) 2 Softmax (èƒŒæ™¯å’Œå‰æ™¯); (2) Regressorsè°ƒæ•´å€™é€‰æ¡† (3) NMS OUT: éæå¤§å€¼æŠ‘åˆ¶(Non-Maximum Suppression)-ä»å¤šä¸ªé‡å çš„è¾¹ç•Œæ¡†ï¼ˆbounding boxesï¼‰ä¸­é€‰æ‹©æœ€ä½³çš„è¾¹ç•Œæ¡†
	3. ROI Layer (Region of Interest Layer): æå–è¿™äº›å€™é€‰åŒºåŸŸçš„ç‰¹å¾
	4. (1)FC1 FC2; (2) c+1 Softmax; (3) Regressors:
8. `fasterrcnn_resnet50_fpn`è¾“å…¥æ˜¯å›¾è±¡åˆ—è¡¨ï¼Œä»£æ›¿batch
9. `backbone`ä½¿ç”¨`resnet`, 5ä¸ªå˜é‡; 1 shape=B*256*168*336; 2 shape=B*256*84*168; å‡åŠ
10. anchor æ˜¯ä¸€ç³»åˆ—é¢„å®šä¹‰çš„çŸ©å½¢æ¡†ï¼ˆæˆ–ç§°ä¸ºå€™é€‰æ¡†ï¼‰ï¼Œå®ƒä»¬ç”¨äºåœ¨å›¾åƒä¸­å®šä½å’Œé¢„æµ‹æ½œåœ¨çš„ç›®æ ‡å¯¹è±¡
11. `RPNHead`: featureå¯¹è¿›è¡Œåˆ†ç±»logitsï¼ˆå‰æ™¯ï¼èƒŒæ™¯ï¼‰å’Œbboxå›å½’
12. shapeå˜åŒ–: Page 13
13. `NMS`: `filter_proposals()` , `rpn.py`
14. `roi_heads`: `MultiScaleRoIAlign:box_roi_pool()`æŠŠä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾ç»Ÿä¸€7*7, `box_head()`, `box_predictor()`
15. Faster RCNN ä¸»è¦ç»„ä»¶(åŒ7)
	1. backbone
	2. RPN
	3. NMS(filter_proposals)
	4. roi_heads
16. è¡Œäººæ£€æµ‹-finetune, [ç›®æ ‡æ£€æµ‹æ¨ègithub](https://github.com/amusi/awesome-object-detection)

## 35 ã€ç¬¬ä¹å‘¨ã€‘ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸€ç¥; 09-01-ppt-ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸€ç¥.pdf
1. GAN(Generative Adversarial Nets): ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ-ä¸€ç§å¯ä»¥ç”Ÿæˆ `ç‰¹å®šåˆ†å¸ƒæ•°æ®` çš„æ¨¡å‹
2. `Generator()`ç”Ÿæˆå™¨(è¾“å…¥å™ªéŸ³)ï¼Œ, `Discriminator()`åˆ¤åˆ«å™¨(äºŒåˆ†ç±»)
3. è®­ç»ƒç›®çš„: (1)å¯¹äºDï¼šå¯¹çœŸæ ·æœ¬è¾“å‡ºé«˜æ¦‚ç‡, (2)å¯¹äºGï¼šè¾“å‡ºä½¿Dä¼šç»™å‡ºé«˜æ¦‚ç‡çš„æ•°æ®
4. ç®—æ³•: æŸå¤±å‡½æ•°ï¼Œå¯¹Dæ˜¯æœ€å¤§æ¢¯åº¦ä¸Šå‡ï¼Œå¯¹Gæ˜¯æœ€å°æ¢¯åº¦ä¸‹é™
5. ç›‘ç£å­¦ä¹ è®­ç»ƒæ¨¡å¼: è®­ç»ƒæ•°æ® -> æ¨¡å‹   -> è¾“å‡ºå€¼ -> æŸå¤±å‡½æ•°(æ ‡ç­¾)  -> å·®å¼‚å€¼
6. GANè®­ç»ƒæ¨¡å¼:      éšæœºæ•°   -> Gæ¨¡å‹ -> è¾“å‡ºå€¼  -> Dæ¨¡å‹(è®­ç»ƒæ•°æ®) -> å·®å¼‚å€¼
7. éƒ½æ˜¯å·ç§¯æ¨¡å‹, Gæ˜¯ä¸€ä¸ªæ”¾å¤§è¿‡ç¨‹(whæ”¾å¤§, Cç¼©å°åˆ°3)ï¼ŒDæ˜¯ä¸€ä¸ªç¼©å°è¿‡ç¨‹ï¼Œ
8. G:(100*1*1) -> (3*64*64); D: (3*64*64) ->(1) 
9. äººè„¸äº”ä¸ªå…³é”®ç‚¹: çœ¼ç›2ï¼Œé¼»å­1ï¼Œå˜´å·´2
10. æé«˜è´¨é‡æªæ–½: (1)æ ‡ç­¾å¹³æ»‘ï¼Œ (2)ç‰¹å¾æ•°é‡ï¼Œ (3)è®­ç»ƒæ•°æ®
11. DCGAN
12. [GANçš„åº”ç”¨](https://jonathan-hui.medium.com/gan-some-cool-applications-of-gans-4c9ecca35900)
13. [GANæ¨è github](https://github.com/nightrome/really-awesome-gan)
14. `gan_demo.py`: çœŸå®æ•°æ®(æ ‡ç­¾é€šå¸¸è®¾ä¸º 1), å‡æ•°æ®ï¼ˆæ ‡ç­¾é€šå¸¸è®¾ä¸º 0ï¼‰; å¼•å…¥æ ‡ç­¾å¹³æ»‘ï¼ˆlabel smoothingï¼‰
15. åˆ¤åˆ«å™¨åˆ†åˆ«å¤„ç†çœŸæ•°æ®å’Œå‡æ•°æ®ï¼Œä¸¤æ¬¡åå‘ä¼ æ’­
16. ç”Ÿæˆå™¨è¾“å…¥çš„æ˜¯éšæœºæ•°ï¼ŒæŸå¤±å‡½æ•°æ˜¯å’ŒçœŸå®æ ‡ç­¾(1)ç›¸æ¯”
17. ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯è®©åˆ¤åˆ«å™¨è®¤ä¸ºå‡æ•°æ®æ˜¯çœŸå®çš„ï¼Œå› æ­¤ä½¿ç”¨ä¸çœŸå®æ•°æ®ç›¸åŒçš„æ ‡ç­¾. è€Œä¸æ˜¯åˆ¤åˆ«å™¨çš„è¾“å‡ºï¼ˆä¸ºäº†ç®€å•?ï¼‰
18. ä¸ºä»€ä¹ˆç”Ÿæˆå™¨çš„æ ‡ç­¾ä¸æ˜¯å’Œåˆ¤åˆ«å™¨çš„è¾“å‡ºç›¸å…³? ä¾‹å¦‚(1-åˆ¤åˆ«å™¨çš„è¾“å‡º)

## 36 ã€ç¬¬ä¹å‘¨ã€‘å¾ªç¯ç¥ç»ç½‘ç»œä¸€ç¥; 09-02-ppt-å¾ªç¯ç¥ç»ç½‘ç»œä¸€ç¥.pdf
1. RNN **å¾ªç¯**ç¥ç»ç½‘ç»œ(Recurrent Neural Networks), ä¸å®šé•¿è¾“å…¥
2. å¸¸ç”¨äº NLP åŠæ—¶é—´åºåˆ—ä»»åŠ¡ï¼ˆè¾“å…¥æ•°æ®å…·æœ‰ å‰åå…³ç³»)
3. RNNç½‘ç»œç»“æ„: ç¥ç»å…ƒçš„è¾“å‡ºS[t] = F(U * X[t] + W * S[t-1]);   æœ€ç»ˆçš„è¾“å‡ºO[t] = SoftMax(V * S[t])
4. å®ç°: å¾ªç¯ï¼Œæœ€åä¸€ä¸ªå¾ªç¯ä½œä¸ºæ¨¡å‹çš„è¾“å‡º
```
    for i in range(line_tensor.size()[0]):		# å•è¯ä¸­çš„æ¯ä¸ªå­—ç¬¦
        output, hidden = rnn(line_tensor[i], hidden)		# y, h = model( [0, 0, â€¦, 1, â€¦, 0], h)
```
5. å­—ç¬¦æ€»é‡: 57 = 26å¤§å†™ + 26å°å†™ + " .,;'"
6. è¾“å…¥: å­—ç¬¦å˜æˆå‘é‡, æ­£äº¤åŸºï¼Œä¸æ˜¯ASCIIå€¼
7. è¾“å‡º: 18åˆ†ç±»
8. å¦‚ä½•å®ç°ä¸å®šé•¿å­—ç¬¦ä¸² åˆ° åˆ†ç±»å‘é‡ çš„æ˜ å°„ï¼Ÿ å¾ªç¯ + å…¨é“¾æ¥å±‚ + æ¿€æ´»å‡½æ•°
9. `rnn_demo.py`: è®­ç»ƒæ•°æ®éšæœº: æ–‡ä»¶åéšæœºï¼Œé‡Œé¢è¡Œéšæœº
10. æ¯ä¸ªå­—ç¬¦è½¬æˆå­—ç¬¦å‘é‡(ä¸€ç»´é•¿åº¦ä¸º57 tensor). ä¸€ä¸ªè¯è½¬æˆä¸‰ç»´å‘é‡: [åå­—ç¬¬å‡ ä¸ªå­—ç¬¦][é•¿åº¦ä¸º1çš„1ç»´][å­—ç¬¦å‘é‡]
11. å›¾è±¡æ˜¯4ç»´å‘é‡: BCHW. è€Œè¿™é‡Œæ˜¯3ç»´
12. `loss = criterion(output, category_tensor)`, output.shape=[1, 18], category_tensor.shape=1

## ä½œä¸š
### ã€ç¬¬ä¸€å‘¨ã€‘[ä½œä¸šè®²è§£1](https://www.jianshu.com/p/5ae644748f21)
1. Scalar æ ‡é‡ï¼Œ0ç»´; Vector ä¸€ç»´æ•°ç»„; Matrix äºŒç»´çŸ©é˜µ; Tensor å¼ é‡
2. `grad_fn`æ˜¯æŒ‡å‘å‡½æ•°çš„æŒ‡é’ˆï¼Œè‡ªåŠ¨æ±‚å¯¼çš„å…³é”®
3. tensor, ndarrayå…±äº«åŒä¸€ä¸ªæ•°æ®å†…å­˜
4. `torch.normal()`, broadcastæœºåˆ¶

### ã€ç¬¬ä¸€å‘¨ã€‘[ä½œä¸šè®²è§£2](https://www.jianshu.com/p/cbce2dd60120)
1. çº¿æ€§å›å½’: ä½¿ç”¨çº¿æ€§å‡½æ•°é€¼è¿‘ç›®æ ‡ï¼Œ`Y=K[0] + K[1]X[1] + ... + K[n]X[n] + e`
2. æ–œç‡Kä¸æ‰°åŠ¨(å¦‚æœæ‰°åŠ¨å°±ä¸èƒ½ç§°ä¸ºçº¿æ€§ï¼Ÿ)ï¼Œåªæ˜¯æˆªè·Xæ‰°åŠ¨
3. è®¡ç®—å›¾: èŠ‚ç‚¹æ˜¯æ•°æ®ï¼Œè¾¹æ˜¯è®¡ç®—
4. åŠ¨æ€å›¾ï¼Œé™æ€å›¾

### ã€ç¬¬ä¸€å‘¨ã€‘ä½œä¸šè®²è§£3
1. é€»è¾‘å›å½’: äºŒåˆ†ç±»é—®é¢˜ï¼ˆbinary classification), å®é™…ä¸Šæ˜¯ä¸€ç§å¹¿ä¹‰çš„çº¿æ€§å›å½’æ¨¡å‹(æå–ç‰¹å¾Kï¼Œç„¶åä½¿ç”¨Sigmoidå‡½æ•°ï¼Œ[0,1])
2. Sigmoid/logisticå‡½æ•°: 1/(1+e^(-z))
3. çº¿æ€§å›å½’(æ ‡é‡)ï¼Œé€»è¾‘å›å½’(æ¦‚ç‡)
4. Sigmoidå‡½æ•°å½“æ•°å€¼å¾ˆå¤§æˆ–å¾ˆå°ï¼Œæ¢¯åº¦æ¶ˆå¤±ï¼Œæ— æ³•è®­ç»ƒ
5. nåˆ†ç±»é—®é¢˜å¸¸ç”¨ç®—æ³•:
	1. é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰: ä¸€å¯¹å¤š
	2. ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰
	3. å†³ç­–æ ‘ï¼ˆDecision Trees
	4. éšæœºæ£®æ—ï¼ˆRandom Forestsï¼‰
	5. æ”¯æŒå‘é‡æœºï¼ˆSupport Vector Machines, SVMï¼‰: è¶…å¹³é¢
	6. Kæœ€è¿‘é‚»ï¼ˆK-Nearest Neighbors, KNNï¼‰
	7. æ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGradient Boosting Decision Trees, GBDTï¼‰
	8. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMultilayer Perceptrons, MLPï¼‰

### ã€ç¬¬äºŒå‘¨ã€‘ä½œä¸šè®²è§£
1. Dataset, DataLoader
```
for i, data in enumerate(train_loader):
class RandomSampler(Sampler[int])
Dataset.__getitem__(index)
```
2. [çŒ«ç‹—äºŒåˆ†ç±»](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data)
3. ç‰¹å¾å›¾å¤§å°å‡åŠ ä¼´éš å·ç§¯æ ¸å¢å€
4. å·ç§¯, BatchNorm, Relu (CBR)ä¸€èµ·
5. å·ç§¯æœ€åï¼Œå…¨é“¾æ¥å±‚å‰ä½¿ç”¨`AdaptiveAvgPool`, `AdaptiveMaxPool`.
6. `AdaptiveMaxPool`ä¸éœ€è¦æŒ‡å®šçª—å£å¤§å°æˆ–æ­¥å¹…ï¼Œè€Œæ˜¯ç›´æ¥æŒ‡å®šè¾“å‡ºçš„ç»´åº¦, å¤„ç†ä¸åŒå°ºå¯¸è¾“å…¥. å¯¹ç‰¹å¾å›¾çš„ä¸€ä¸ªå¹³é¢(w,h)æ±‚Max, æœ€åæŠŠé€šé“Catèµ·æ¥. 
	1. å¥½å¤„ (1)å¤§å¤§å‡å°‘å‚æ•°ä¸ªæ•°(å¹³é¢->2ç‚¹), (2)æ­£åˆ™åŒ–(æŠ½è±¡:å¹³å‡ï¼Œæœ€å¤§), (3)æ–¹ä¾¿è°ƒèŠ‚ç½‘ç»œè¾“å…¥å›¾ç‰‡çš„å¤§å°
	2. å·ç§¯å’Œå…¨é“¾æ¥çš„è¿‡åº¦ï¼Œåªå…³å¿ƒæ ¸çš„ä¸ªæ•°ï¼Œè·Ÿç‰¹å¾å›¾å¤§å°æ— å…³
7. ä¼˜åŒ–: æ•°æ®ä¼˜åŒ–ï¼ˆè£å‰ªï¼Œæ˜ å°„ï¼‰; æ¨¡å‹
8. å…ˆå†™æ¨¡å‹ç±»ï¼Œå†å†™è®­ç»ƒç±»
9. [ä½œä¸šä»£ç ](https://github.com/greebear/pytorch-learning)

### ã€ç¬¬ä¸‰å‘¨ã€‘ä½œä¸šè®²è§£
1. `Sequential`: ç®€å•çš„çº¿æ€§æµæ°´çº¿. è‡ªåŠ¨æ‰§è¡Œå­æ¨¡å—çš„å‰å‘ä¼ æ’­; é™åˆ¶æ˜¯ä¸å…è®¸å¤šè¾“å…¥ã€å¤šè¾“å‡ºæˆ–éœ€è¦åˆ†æ”¯çš„æ¨¡å‹ã€‚
2. `ModuleList`: æ˜ç¡®çš„é¡ºåºï¼Œé€šè¿‡ç´¢å¼•è¿­ä»£. éœ€è¦åœ¨çˆ¶æ¨¡å—çš„forwardæ–¹æ³•ä¸­æ‰‹åŠ¨è°ƒç”¨æ¯ä¸ªå­æ¨¡å—, ä»…åˆ—è¡¨å®¹å™¨
3. `ModuleDict`: é€šè¿‡åç§°è®¿é—®ï¼Œæ²¡æœ‰æ˜ç¡®çš„é¡ºåº
4. `class Module`: 8 ä¸ªå­—å…¸ç®¡ç†å®ƒçš„å±æ€§: parameters, modules, buffers, ***_hooks(å…±5ä¸ªé’©å­)
5. `class _ConvNd(Module)`: stride, in_channels, out_channels, kernel_size...
6. `class Conv2d(_ConvNd)`: æ€ä¹ˆè¿›è¡Œæ¢¯åº¦ä¼ æ’­`forward()`
7. `class Linear(Module)`: self.weight, self.bias
8. pythonçš„è”åˆ: typing.Union[int, typing.Tuple[int, int]]
9. `torchvision.models.AlexNet()._modules['features']._modules.keys()`
	1. `AlexNet()._modules`æ˜¯`module`çš„å­—å…¸å˜é‡
	2. 'features'æ˜¯`AlexNet()`çš„ç‰¹å¾æå–å±‚: features, avgpool, classifier
	3. æœ€åçš„`_modules`æ˜¯ç‰¹å¾æå–å±‚çš„æ¯ä¸€ä¸ªåŸºæœ¬æ­¥éª¤
10. `ModuleDict()`åŠ ä¸Šäº†åå­—, ä½†æ˜¯å¤±å»äº†é¡ºåº`forward()`ï¼Œæ‰€ä»¥ä½¿ç”¨`Sequential(OrderedDict({}))`ä»£æ›¿`ModuleDict()`
11. å½“ä¿®æ”¹å±‚æ—¶ï¼Œä½¿ç”¨åå­—æŠŠç´¢å¼•æ–¹ä¾¿
12. è½¬ç½®å·ç§¯(Transpose Convolution): åå·ç§¯ï¼ˆDeconvolutionï¼‰:  å°†æ•°æ®çš„ç©ºé—´ç»´åº¦ä»è¾ƒå°å°ºå¯¸æ˜ å°„åˆ°è¾ƒå¤§å°ºå¯¸, å®ç°ç‰¹å¾å›¾å°ºå¯¸çš„æ‰©å¤§(ä¸Šé‡‡æ ·ï¼ˆupsamplingï¼‰)
13. è½¬ç½®å·ç§¯å®ç°: æ’å…¥é›¶å¡«å……ï¼ˆZero Insertionï¼‰, è¾“å‡ºå¡«å……ï¼ˆOutput Paddingï¼‰
14. è½¬ç½®å·ç§¯åº”ç”¨: å›¾åƒåˆ†å‰², ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
15. åæ± åŒ–(MaxUnpool): åœ¨è¿›è¡Œæœ€å¤§æ± åŒ–æ—¶ï¼Œè®°å½•æœ€å¤§å€¼çš„ä½ç½®, æ¢å¤æ—¶å…¶ä»–ä½ç½®åˆ™å¡«å……ä¸ºé›¶
16. æ¿€æ´»å‡½æ•°: ReLuåŠå˜ä½“, Sigmoid, Tanh
17. `nn.Conv2d(in_channels, out_channels...)`: out_channelså…±æœ‰å‡ ä¸ªå·ç§¯è¿›è¡Œè¿ç®—ï¼Œin_channelsæ˜¯æ¯ä¸ªå·ç§¯æœ‰å‡ ä¸ªåˆ†ç‰‡, `conv_layer1.weight.shape`
18. å·ç§¯æ˜¯å¯¹åº”ä½ç½®ç›¸ä¹˜ç„¶åç›¸åŠ å¾—åˆ°ä¸€ä¸ªå…ƒç´ , å’ŒçŸ©é˜µä¹˜æ³•ç¬¬ä¸€æ­¥ä¸€æ ·
	1. ä¸€ä¸ªå·ç§¯æ ¸å¯¹åº”è¾“å…¥æ¯ä¸ªé€šé“éƒ½æœ‰ä¸€ä¸ªåˆ†ç‰‡, è¾“å‡ºä¸€ä¸ªç‰¹å¾å›¾, ä½œä¸ºåé¢è¾“å…¥çš„ä¸€ä¸ªé€šé“
	2. é€šé“0å’Œå·ç§¯æ ¸0åˆ†ç‰‡è®¡ç®—ï¼Œå½¢æˆç‰¹å¾å›¾åˆ†ç‰‡0-0
	3. é€šé“1å’Œå·ç§¯æ ¸1åˆ†ç‰‡è®¡ç®—ï¼Œå½¢æˆç‰¹å¾å›¾åˆ†ç‰‡0-1
	4. é€šé“2å’Œå·ç§¯æ ¸2åˆ†ç‰‡è®¡ç®—ï¼Œå½¢æˆç‰¹å¾å›¾åˆ†ç‰‡0-2
	4. è¾“å‡ºç‰¹å¾å›¾ = ç‰¹å¾å›¾åˆ†ç‰‡0-0 + ç‰¹å¾å›¾åˆ†ç‰‡0-1 + ç‰¹å¾å›¾åˆ†ç‰‡0-2 + bias
19. 2Då·ç§¯ç”¨äºå›¾ç‰‡B*C*H*Wï¼Œ3Då·ç§¯ç”¨äºè§†é¢‘B*C*D*H*W

### ã€ç¬¬å››å‘¨ã€‘ä½œä¸šè®²è§£
1. æ¢¯åº¦æ¶ˆå¤±ï¼çˆ†ç‚¸: æ¢¯åº¦å‰ªåˆ‡ï¼Œæƒé‡æ­£åˆ™åŒ–ï¼Œæ¿€æ´»å‡½æ•°æ”¹è¿›ï¼Œbatchnorm, ResNet
2. æ¢¯åº¦å‰ªåˆ‡: è¶…è¿‡äº†é˜ˆå€¼ï¼Œç¼©æ”¾åˆ°è¯¥é˜ˆå€¼ä»¥ä¸‹
3. ResNet: æ¯å±‚å­¦ä¹ çš„æ˜¯ç›®æ ‡æ˜ å°„ä¸è¾“å…¥ä¹‹é—´çš„æ®‹å·® (å·®å¼‚); æ®‹å·®å—è¶‹å‘äºé›¶; æœºåˆ¶(æ¢¯åº¦ç›´æ¥ä¼ é€’, æ’ç­‰æ˜ å°„, ç®€åŒ–å­¦ä¹ ç›®æ ‡)
4. å…¶å®ƒç½‘ç»œå­¦ä¹ çš„æ˜¯è¾“å…¥ -> ç›®æ ‡ï¼Œå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±ï¼çˆ†ç‚¸é—®é¢˜
5. æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–ç®—æ³•

### ã€ç¬¬äº”å‘¨ã€‘ä½œä¸šè®²è§£: å­¦ä¹ ç‡
1. Tensorboard
2. [anacondaå®‰è£…åŒ…](https://anaconda.cloud/)

### ã€ç¬¬å…­å‘¨ã€‘ä½œä¸šè®²è§£: æ­£åˆ™åŒ–
1. [CS231n: CNN for Visual Recognition.](https://cs231n.github.io/)


### ã€ç¬¬ä¸ƒå‘¨ã€‘ä½œä¸šè®²è§£: æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½
1. `torch.save()`, `torch.load()`
2. [API DOC](https://pytorch.org/docs/stable/torch.html)
3. æ¨¡å‹å¾®è°ƒï¼ˆFinetuneï¼‰: æ¨¡å‹å¾®è°ƒæ˜¯è¿ç§»å­¦ä¹ çš„ä¸€éƒ¨åˆ†; å…±äº«åŒä¸€ä¸ªç›®æ ‡ï¼Œå³åˆ©ç”¨å·²æœ‰çš„çŸ¥è¯†å’Œæ•°æ®æ¥æé«˜æ–°ä»»åŠ¡çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½
4. [Transfer Learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)
5. [Finetuning Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)
6. GPU: `torch.nn.DataParallel`, `torch.cuda`
7. [Distributed and Parallel Training Tutorials](https://pytorch.org/tutorials/distributed/home.html)
8. ssh connect Remote Notebook


### æ•°æ®æº
1. [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
2. [ImageNet](https://image-net.org/)
3. [ImageFolder](https://www.tugraz.at/institute/icg/home)
4. [LSUN Classification](https://www.cs.princeton.edu/)
5. [COCO (Captioning and Detection)](http://mscoco.org/)
6. [kaggle](www.kaggle.com)

## ä¸ºä»€ä¹ˆè¦ä½¿ç”¨`1*1`å·ç§¯?
1. é™ç»´åº¦å’Œå‡ç»´åº¦ - å·ç§¯ç‰¹ç‚¹
2. éçº¿æ€§å˜æ¢, å› ä¸ºåé¢åº”ç”¨ï¼ˆå¦‚ReLUï¼‰
3. å‚æ•°å…±äº«: 1x1å·ç§¯æ ¸çš„ä½¿ç”¨å‡å°‘äº†å‚æ•°çš„æ•°é‡ï¼Œå› ä¸ºå®ƒåªåœ¨é€šé“ä¹‹é—´å…±äº«æƒé‡ï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªè¾“å…¥å›¾åƒä¸Šã€‚è¿™æœ‰åŠ©äºå‡å°æ¨¡å‹çš„å°ºå¯¸ï¼Œé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå¹¶æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡
4. è®¡ç®—æ•ˆç‡

## Read Image, convert to Tensor
```
from PIL import Image

img = Image.open(path_img).convert('RGB')`

img_transform = transforms.Compose([transforms.ToTensor()])
img_tensor = img_transform(img)		# C*H*W
img_tensor.unsqueeze_(dim=0)    	# C*H*W to B*C*H*W, [1 * 3 * 512 * 512]

# ç»è¿‡ 3*3 å·ç§¯
conv_layer = nn.Conv2d(3, 1, 3)     # input:(i, o, size) weights:(o, i , h, w)
nn.init.xavier_normal_(conv_layer.weight.data)
img_conv = conv_layer(img_tensor)   # B*C*H*W [1, 1, 510, 510]

# è½¬å›å›¾è±¡
img_raw  = transform_invert(img_tensor.squeeze(),  img_transform)   # img_tensor.shape=[1, 3, 512, 512]
img_conv = transform_invert(img_conv[0, 0:1, ...], img_transform)   # img_conv.shape  =[1, 1, 510, 510]
```

## slice
```
import numpy as np
img_conv = np.random.random((3, 4, 5))
img_conv.shape		# (3, 4, 5)

result = img_conv[0, ...]
result.shape		# (4, 5)

result = img_conv[0:2, ...]
result.shape		# (2, 4, 5)

result = img_conv[0, 0:1, ...]
result.shape		# (1, 5)

result = img_conv[0, 0:1, :]
result.shape		# (1, 5)
```

## plt
```
plt.cla()   # é˜²æ­¢ç¤¾åŒºç‰ˆå¯è§†åŒ–æ—¶æ¨¡å‹é‡å 
plt.plot(x, y, 'r-', lw=5)	# `r-'è¡¨ç¤ºçº¢è‰²ï¼ˆrï¼‰çš„å®çº¿ï¼ˆ-ï¼‰	# lw=5: è¡¨ç¤ºçº¿çš„å®½åº¦5
plt.text(2, 20, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'red'})
plt.xlim(1.5, 10)
plt.title("title")

plt.show(block=True)
plt.pause(0.5)
```

## `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)`
1. `out_channels ï¼šè¾“å‡ºé€šé“æ•°ï¼Œç­‰ä»·äºå·ç§¯æ ¸ä¸ªæ•°`????
2. å·ç§¯æ ¸è¿ç®—æ¬¡æ•°Wæ–¹å‘: `å¤„ç†æ¬¡æ•°=(å¤„ç†å‰w - æ ¸çš„w) / stride + 1`
3. å› ä¸ºæ¯æ¬¡è¿ç®—äº§ç”Ÿä¸€ä¸ªå•å…ƒï¼Œæ‰€ä»¥å¤„ç†åWç»´æ•°ç­‰äºå·ç§¯æ ¸è¿ç®—æ¬¡æ•°
4. å·ç§¯ç»´åº¦ï¼šä¸€èˆ¬æƒ…å†µä¸‹ ï¼Œå·ç§¯æ ¸åœ¨å‡ ä¸ªç»´åº¦ä¸Šæ»‘åŠ¨ï¼Œå°±æ˜¯å‡ ç»´å·ç§¯
5. 1ç»´å·ç§¯æ ¸æ˜¯ä¸€ç»´, 2ç»´å·ç§¯æ ¸æ˜¯äºŒç»´æ¯”å¦‚(5*5), 3ç»´å·ç§¯æ ¸æ˜¯ä¸‰ç»´æ¯”å¦‚(3*3*3)
6. å¦‚æœè¦ä¿æŒå‰åç»´åº¦ä¸å˜ï¼Œéœ€è¦å¡«å……`æ ¸çš„w-1`ä¸ªæ ¼å­; å››ä¸ªæ–¹å‘ï¼ˆå‰åï¼‰éƒ½å¡«å……(å› ä¸ºæ ¸æ˜¯å¥‡æ•°ï¼Œæ‰€ä»¥å‰åå¡«å……ç›¸ç­‰)
7. å­”æ´å·ç§¯å¯ä»¥çœ‹æˆå·ç§¯æ ¸æ‰©å¤§ï¼Œ(3*3) -> (5*5)

## å‡½æ•°å‚æ•°
```
kernel_size = 3
kernel_size_ = _pair(kernel_size)		# (3, 3)

padding: Union[str, _size_2_t] = 0
padding_ = padding if isinstance(padding, str) else _pair(padding)

padding_mode: str = 'zeros'
```